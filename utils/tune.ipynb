{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TUNER PROTOTYPES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits, fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "from scipy.stats import  zscore\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Integer, Real, Categorical\n",
    "from skopt.plots import plot_objective, plot_convergence, plot_histogram, plot_gaussian_process\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Data Processing\n",
    "digits = load_digits()\n",
    "\n",
    "cal_housing = fetch_california_housing(as_frame=True)\n",
    "\n",
    "cleanData = cal_housing.data\n",
    "cleanData['y'] = cal_housing.target\n",
    "\n",
    "cleanData = cleanData.drop(columns=['Longitude', 'Latitude'])\n",
    "\n",
    "for feature in ['AveBedrms', 'AveRooms', 'AveOccup', 'Population']:\n",
    "    cleanData = cleanData[(np.abs(zscore(cleanData[feature])) < 2.5)]\n",
    "cleanTarget = cleanData['y'].to_list()\n",
    "\n",
    "cleanData.drop(columns=['y'], inplace=True)\n",
    "\n",
    "modelData = cleanData.copy()\n",
    "\n",
    "\n",
    "# TODO: Add GridSearchCV into Pipeline for testing scoringCriteria param space?\n",
    "# TODO: Generate SKOPT Plots for regression models, Plot Feature Importance for all models\n",
    "# TODO: Insert Outlier script into Pipeline\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class ModelTuner:\n",
    "    def __init__(self, classifiers_param_spaces, scoring_criteria):\n",
    "        self.classifiers_param_spaces = classifiers_param_spaces\n",
    "        self.scoring_criteria = scoring_criteria\n",
    "        self.tuned_classifiers = {}\n",
    "\n",
    "    def tune_classifier(self, search_type, model_name, model, param_space, X_train, y_train, X_test, y_test):\n",
    "        \"\"\"Tune the classifier using the specified search type.\"\"\"\n",
    "        pipe = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('pca', PCA()),\n",
    "            ('classify', model())\n",
    "        ])\n",
    "        \n",
    "        if search_type == 'bayesian':\n",
    "            search = BayesSearchCV(\n",
    "                estimator=pipe,\n",
    "                search_spaces=param_space,\n",
    "                n_iter=50,\n",
    "                cv=4,\n",
    "                scoring= None,  # Default scoring\n",
    "                n_jobs=-1,\n",
    "                random_state=42\n",
    "            )\n",
    "        \n",
    "        for scoring_metric in self.scoring_criteria['classification']:\n",
    "            print(f\"Tuning {model_name} with {search_type} search and scoring metric: {scoring_metric}\")\n",
    "            search.set_params(scoring=scoring_metric)\n",
    "            search.fit(X_train, y_train)\n",
    "\n",
    "            print(f\"Best Params: {search.best_params_}\")\n",
    "            print(f\"Best Score (Train): {search.best_score_}\")\n",
    "            test_score = search.score(X_test, y_test)\n",
    "            print(f\"Test Score: {test_score}\")\n",
    "            print(\"-\" * 80)\n",
    "\n",
    "            self.plot_confusion_matrix(search.best_estimator_, X_test, y_test)\n",
    "            self.plot_optimization_results(search)\n",
    "\n",
    "            # self.plot_regression_results(search.best_estimator_, X_test, y_test)\n",
    "\n",
    "        self.tuned_classifiers[model_name] = search\n",
    "        \n",
    "\n",
    "        return search\n",
    "\n",
    "    def evaluate_model(self, model_name, X_test, y_test):\n",
    "        \"\"\"Evaluate the tuned model and display confusion matrix.\"\"\"\n",
    "        search = self.tuned_classifiers.get(model_name)\n",
    "        if not search:\n",
    "            raise ValueError(f\"Model {model_name} not tuned yet!\")\n",
    "\n",
    "        y_pred = search.best_estimator_.predict(X_test)\n",
    "        print(f\"Classification Report for {model_name}:\\n{classification_report(y_test, y_pred)}\")\n",
    "\n",
    "    def tune_all_classifiers(self, search_type, X_train, y_train, X_test, y_test):\n",
    "        \"\"\"Tune all classifiers in the parameter spaces.\"\"\"\n",
    "        for model_name, model_dict in self.classifiers_param_spaces.items():\n",
    "            if search_type in model_dict:\n",
    "                param_space = model_dict[search_type]\n",
    "                model = globals()[model_name]\n",
    "                self.tune_classifier(search_type, model_name, model, param_space, X_train, y_train, X_test, y_test)\n",
    "\n",
    "    def plot_confusion_matrix(self, best_estimator, X_test, y_test):\n",
    "        y_pred = best_estimator.predict(X_test)\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "        disp.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        plt.show()\n",
    "\n",
    "    def plot_optimization_results(self, search):\n",
    "        \"\"\"Plot optimization-related visualizations.\"\"\"\n",
    "        # Plot convergence\n",
    "        # ax = plot_convergence(search.optimizer_results_[0])\n",
    "        # plt.title(\"Optimization Convergence\")\n",
    "        # plt.subplots_adjust(wspace=0.5, hspace=0.5)\n",
    "        # plt.show()\n",
    "\n",
    "        # Plot objective function\n",
    "        ax = plot_objective(search.optimizer_results_[0], n_minimum_search=int(1e8))\n",
    "        plt.title(\"Objective Function\")\n",
    "        plt.subplots_adjust(wspace=0.5, hspace=0.5)\n",
    "        plt.show()\n",
    "\n",
    "        # Plot hyperparameter distribution\n",
    "        # ax = plot_histogram(dimension_identifier=search.optimizer_results_[0])\n",
    "        # plt.title(\"Hyperparameter Sampling Distribution\")\n",
    "        # plt.show()\n",
    "    '''\n",
    "    def plot_regression_results(self, best_estimator, X_test, y_test):\n",
    "        \"\"\"Plot regression results.\"\"\"\n",
    "        y_pred = best_estimator.predict(X_test)\n",
    "        plt.scatter(y_test, y_pred, alpha=0.7, edgecolors='k')\n",
    "        plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "        plt.xlabel(\"Actual\")\n",
    "        plt.ylabel(\"Predicted\")\n",
    "        plt.title(\"Regression Results\")\n",
    "        plt.show()\n",
    "    '''\n",
    "\n",
    "# Define scoring criteria\n",
    "scoring_criteria = {\n",
    "    'classification': ['accuracy', 'f1_macro', 'balanced_accuracy', 'precision_macro', 'recall_macro', 'average_precision']\n",
    "}\n",
    "\n",
    "# Classifiers parameter spaces\n",
    "classifier_param_spaces = {\n",
    "    'DecisionTreeClassifier': {\n",
    "        'bayesian': {\n",
    "            'classify__criterion': Categorical(['gini', 'entropy', 'log_loss']),\n",
    "            'classify__splitter': Categorical(['best', 'random']),\n",
    "            'classify__max_depth': Integer(1, 1000),\n",
    "            'classify__min_samples_split': Real(0.01, 0.9),\n",
    "            'classify__min_samples_leaf': Real(0.01, 0.9),\n",
    "            'classify__max_features': Real(0.01, 0.9),\n",
    "            'classify__max_leaf_nodes': Integer(2, 4000),\n",
    "            'classify__min_impurity_decrease': Real(0.0, 1.0),\n",
    "            'classify__ccp_alpha': Real(0.01, 0.9),\n",
    "            'pca__n_components': Integer(1, len(load_digits().data[0])),\n",
    "            'scaler__with_mean': [True, False],\n",
    "            'scaler__with_std': [True, False]\n",
    "        }\n",
    "    },\n",
    "    'RandomForestClassifier': {\n",
    "        'bayesian': {\n",
    "            'classify__n_estimators': Integer(10, 2000),\n",
    "            'classify__criterion': Categorical(['gini', 'entropy', 'log_loss']),\n",
    "            'classify__max_depth': Integer(1, 1000),\n",
    "            'classify__min_samples_split': Real(0.01, 0.9),\n",
    "            'classify__min_samples_leaf': Real(0.01, 0.9),\n",
    "            'classify__max_features': Real(0.01, 0.9),\n",
    "            'classify__max_leaf_nodes': Integer(2, 4000),\n",
    "            'classify__min_impurity_decrease': Real(0.0, 1.0),\n",
    "            'classify__oob_score': Categorical([True, False]),\n",
    "            'classify__warm_start': Categorical([True, False]),\n",
    "            'classify__max_samples': Real(0.01, 0.9),\n",
    "            'pca__n_components': Integer(1, len(load_digits().data[0])),\n",
    "            'scaler__with_mean': [True, False],\n",
    "            'scaler__with_std': [True, False]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Load dataset and split\n",
    "digits = load_digits()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.3, random_state=42)\n",
    "\n",
    "# Instantiate and run\n",
    "tuner = ModelTuner(classifier_param_spaces, scoring_criteria)\n",
    "tuner.tune_all_classifiers('bayesian', X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Evaluate a specific model\n",
    "tuner.evaluate_model('DecisionTreeClassifier', X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLD REG/CLF CODE FROM SUBMISSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "TODO: Build a class, model_tuner(), to run BayesianSearch optmization to test predefined param search criteria for each model \n",
    "        --> return printed evaluation and scoring report, plot confusion matrix for all models\n",
    "\n",
    "'''\n",
    "\n",
    "# Define scoring criteria\n",
    "scoring_criteria = {\n",
    "    'classification': ['accuracy', 'f1_macro', 'balanced_accuracy', 'precision_macro', 'recall_macro', 'average_precision']\n",
    "}\n",
    "\n",
    "# Classifiers parameter spaces for GridSearchCV, RandomizedSearchCV, and BayesSearchCV\n",
    "classifier_param_spaces = {\n",
    "    'DecisionTreeClassifier': {\n",
    "        'bayesian': {\n",
    "            'model__criterion': Categorical(['gini', 'entropy', 'log_loss']),\n",
    "            'model__splitter': Categorical(['best', 'random']),\n",
    "            'classify__max_depth': Integer(1, 1000),\n",
    "            'model__min_samples_split': Real(0.01, 0.9),\n",
    "            'classify__min_samples_leaf': Real(0.01, 0.9),\n",
    "            'classify__max_features': Real(0.01, 0.9),\n",
    "            'classify__max_leaf_nodes': Integer(2, 4000),\n",
    "            'classify__min_impurity_decrease': Real(0.0, 1.0),\n",
    "            'classify__ccp_alpha': Real(0.01, 0.9),\n",
    "            'pca__n_components': Integer(1, len(digits.data[0])),\n",
    "            'scaler__with_mean': [True, False],\n",
    "            'scaler__with_std': [True, False]\n",
    "        }\n",
    "    },\n",
    "\n",
    "    'RandomForestClassifier': {\n",
    "        'bayesian': {\n",
    "            'classify__n_estimators': Integer(10, 2000),\n",
    "            'classify__criterion': Categorical(['gini', 'entropy', 'log_loss']),\n",
    "            'classify__max_depth': Integer(1, 1000),\n",
    "            'classify__min_samples_split': Real(0.01, 0.9),\n",
    "            'classify__min_samples_leaf': Real(0.01, 0.9),\n",
    "            'classify__max_features': Real(0.01, 0.9),\n",
    "            'classify__max_leaf_nodes': Integer(1, 2000),\n",
    "            'classify__min_impurity_decrease': Real(0.01, 0.9),\n",
    "            'classify__oob_score': Categorical([True, False]),\n",
    "            'classify__warm_start': Categorical([True, False]),\n",
    "            'classify__max_samples': Real(0.01, 0.9),\n",
    "            'pca__n_components': Integer(1, len(digits.data[0])),\n",
    "            'scaler__with_mean': [True, False],\n",
    "            'scaler__with_std': [True, False]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "def tune_classifier(search_type, model_name, model, param_space, X_train, y_train, X_test, y_test):\n",
    "    pipe = Pipeline([ \n",
    "        ('scaler', StandardScaler()), \n",
    "        ('pca', PCA()), \n",
    "        ('classify', model()) \n",
    "    ])\n",
    "\n",
    "    # Set search class based on the search_type\n",
    "    if search_type == 'bayesian':\n",
    "        search_class = BayesSearchCV\n",
    "        search_params = {\n",
    "            'estimator': pipe,\n",
    "            'search_spaces': param_space,  # BayesSearchCV uses search_spaces\n",
    "            'n_iter': 50,  # Define number of iterations for Bayesian Optimization\n",
    "            'cv': 4,\n",
    "            'scoring': 'accuracy',\n",
    "            'n_jobs': -1,\n",
    "            'random_state': 42\n",
    "        }\n",
    "\n",
    "    search = search_class(**search_params)\n",
    "\n",
    "    # Loop over scoring metrics for evaluation\n",
    "    for scoring_metric in scoring_criteria['classification']:\n",
    "        print(f'Tuning {model_name} with {search_type} search and scoring metric: {scoring_metric}')\n",
    "        search.set_params(scoring=scoring_metric)\n",
    "        search.fit(X_train, y_train)\n",
    "        \n",
    "        print(f'Evaluation Metric: {scoring_metric}')\n",
    "        print(f\"Best Score (Train): {search.best_score_}\")\n",
    "        print(f\"Test Score: {search.score(X_test, y_test)}\")\n",
    "        print('-' * 80)\n",
    "\n",
    "    return search\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.3, random_state=42)\n",
    "\n",
    "# Instantiate tuned models dictionary\n",
    "tuned_classifiers = {}\n",
    "\n",
    "# Iterate through each classifier and perform tuning\n",
    "for model_name, model_dict in classifier_param_spaces.items():\n",
    "    # Run BayesianSearchCV first\n",
    "    if 'bayesian' in model_dict:\n",
    "        param_space = model_dict['bayesian']\n",
    "        search = tune_classifier('bayesian', model_name, globals()[model_name], param_space, X_train, y_train, X_test, y_test)\n",
    "        tuned_classifiers[model_name] = search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "TODO: Build a class, model_tuner(), to run BayesianSearch optmization to test predefined param search criteria for each model \n",
    "        --> return printed evaluation and scoring report, plot confusion matrix for all models\n",
    "\n",
    "'''\n",
    "\n",
    "# Define scoring criteria\n",
    "scoring_criteria = {\n",
    "    'regression': [\n",
    "        'neg_mean_squared_error', 'r2', 'neg_mean_absolute_error', 'neg_mean_squared_log_error', \n",
    "        'neg_median_absolute_error', 'max_error', 'explained_variance', 'neg_root_mean_squared_error'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Regressors parameter spaces for GridSearchCV and RandomizedSearchCV\n",
    "regression_param_spaces = {\n",
    "    'DecisionTreeRegressor': {\n",
    "        'bayesian': {\n",
    "            'regress__criterion': ['squared_error', 'friedman_mse', 'absolute_error', 'poisson'],\n",
    "            'regress__splitter': ['best', 'random'],\n",
    "            'regress__max_depth': Integer(2, 101),\n",
    "            'regress__min_samples_split': Real(0.01, 0.89),\n",
    "            'regress__min_samples_leaf': Real(0.01, 0.89),\n",
    "            'regress__max_features': Real(0.01, 0.49),\n",
    "            'regress__ccp_alpha': Real(0.01, 0.89),\n",
    "            'pca__n_components': Integer(1, len(modelData.columns)),\n",
    "            'scaler__with_mean': [True, False],\n",
    "            'scaler__with_std': [True, False]\n",
    "        }\n",
    "    },\n",
    "    'RandomForestRegressor': {\n",
    "        'bayesian': {\n",
    "            'regress__criterion': ['squared_error', 'friedman_mse', 'absolute_error', 'poisson'],\n",
    "            'regress__n_estimators': Integer(50, 501),\n",
    "            'regress__max_depth': Integer(2, 101),\n",
    "            'regress__min_samples_split': Real(0.01, 0.89),\n",
    "            'regress__min_samples_leaf': Real(0.01, 0.89),\n",
    "            'regress__max_features': Real(0.01, 0.89),\n",
    "            'regress__ccp_alpha': Real(0.01, 0.89),\n",
    "            'regress__max_samples': Real(0.01, 0.89),\n",
    "            'pca__n_components': Integer(1, len(modelData.columns)),\n",
    "            'scaler__with_mean': [True, False],\n",
    "            'scaler__with_std': [True, False]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "def tune_classifier(search_type, model_name, model, param_space, X_train, y_train, X_test, y_test):\n",
    "    pipe = Pipeline([ \n",
    "        ('scaler', StandardScaler()), \n",
    "        ('pca', PCA()), \n",
    "        ('regress', model()) \n",
    "    ])\n",
    "\n",
    "    # Set search class based on the search_type\n",
    "    if search_type == 'bayesian':\n",
    "        search_class = BayesSearchCV\n",
    "        search_params = {\n",
    "            'estimator': pipe,\n",
    "            'search_spaces': param_space,  # BayesSearchCV uses search_spaces\n",
    "            'n_iter': 50,  # Define number of iterations for Bayesian Optimization\n",
    "            'cv': 4,\n",
    "            'scoring': 'accuracy',\n",
    "            'n_jobs': -1,\n",
    "            'random_state': 42\n",
    "        }\n",
    "\n",
    "    search = search_class(**search_params)\n",
    "\n",
    "    # Loop over scoring metrics for evaluation\n",
    "    for scoring_metric in scoring_criteria['regression']:\n",
    "        print(f'Tuning {model_name} with {search_type} search and scoring metric: {scoring_metric}')\n",
    "        search.set_params(scoring=scoring_metric)\n",
    "        search.fit(X_train, y_train)\n",
    "        \n",
    "        print(f'Evaluation Metric: {scoring_metric}')\n",
    "        print(f\"Best Score (Train): {search.best_score_}\")\n",
    "        print(f\"Test Score: {search.score(X_test, y_test)}\")\n",
    "        print('-' * 80)\n",
    "\n",
    "    return search\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(modelData, cleanTarget, test_size=0.3, random_state=42)\n",
    "\n",
    "# Instantiate tuned models dictionary\n",
    "tuned_classifiers = {}\n",
    "\n",
    "# Iterate through each classifier and perform tuning\n",
    "for model_name, model_dict in regression_param_spaces.items():\n",
    "    # Run BayesianSearchCV first\n",
    "    if 'bayesian' in model_dict:\n",
    "        param_space = model_dict['bayesian']\n",
    "        search = tune_classifier('bayesian', model_name, globals()[model_name], param_space, X_train, y_train, X_test, y_test)\n",
    "        tuned_classifiers[model_name] = search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ATTEMPTED COMBINED TUNER (DOESN'T WORK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits, fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, mean_squared_error, r2_score\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Integer, Real, Categorical\n",
    "from scipy.stats import zscore\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class ModelTuner:\n",
    "    def __init__(self, models_param_spaces, scoring_criteria):\n",
    "        self.models_param_spaces = models_param_spaces\n",
    "        self.scoring_criteria = scoring_criteria\n",
    "        self.tuned_models = {}\n",
    "\n",
    "    def tune_model(self, task_type, search_type, model_name, model, param_space, X_train, y_train, X_test, y_test):\n",
    "        \"\"\"Tunes the model using the specified search type.\"\"\"\n",
    "        pipe = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('pca', PCA()),\n",
    "            ('model', model())\n",
    "        ])\n",
    "        \n",
    "        # Initialize search object\n",
    "        if search_type == 'bayesian':\n",
    "            search = BayesSearchCV(\n",
    "                estimator=pipe,\n",
    "                search_spaces=param_space,\n",
    "                n_iter=50,\n",
    "                cv=4,\n",
    "                scoring=None,  # Will set dynamically\n",
    "                n_jobs=-1,\n",
    "                random_state=42\n",
    "            )\n",
    "        \n",
    "        # Loop over scoring metrics\n",
    "        for scoring_metric in self.scoring_criteria[task_type]:\n",
    "            print(f\"Tuning {model_name} ({task_type}) with {search_type} search and scoring metric: {scoring_metric}\")\n",
    "            search.set_params(scoring=scoring_metric)\n",
    "            search.fit(X_train, y_train)\n",
    "\n",
    "            print(f\"Best Params: {search.best_params_}\")\n",
    "            print(f\"Best Score (Train): {search.best_score_}\")\n",
    "            test_score = search.score(X_test, y_test)\n",
    "            print(f\"Test Score: {test_score}\")\n",
    "            print(\"-\" * 80)\n",
    "\n",
    "            self.plot_confusion_matrix(search.best_estimator_, X_test, y_test)\n",
    "            \n",
    "            if task_type == 'regression':\n",
    "                self.plot_regression_results(search.best_estimator_, X_test, y_test)\n",
    "\n",
    "        # Save the tuned model\n",
    "        self.tuned_models[model_name] = search\n",
    "\n",
    "        return search\n",
    "\n",
    "    def tune_all_models(self, task_type, search_type, X_train, y_train, X_test, y_test):\n",
    "        \"\"\"Tunes all models in the parameter spaces.\"\"\"\n",
    "        for model_name, model_dict in self.models_param_spaces.items():\n",
    "            if search_type in model_dict:\n",
    "                param_space = model_dict[search_type]\n",
    "                model = globals()[model_name]\n",
    "                self.tune_model(task_type, search_type, model_name, model, param_space, X_train, y_train, X_test, y_test)\n",
    "\n",
    "    def evaluate_model(self, model_name, task_type, X_test, y_test):\n",
    "        \"\"\"Evaluate the tuned model and display relevant results.\"\"\"\n",
    "        search = self.tuned_models.get(model_name)\n",
    "        if not search:\n",
    "            raise ValueError(f\"Model {model_name} not tuned yet!\")\n",
    "\n",
    "        y_pred = search.best_estimator_.predict(X_test)\n",
    "\n",
    "        if task_type == 'classification':\n",
    "            print(f\"Classification Report for {model_name}:\\n{classification_report(y_test, y_pred)}\")\n",
    "        elif task_type == 'regression':\n",
    "            mse = mean_squared_error(y_test, y_pred)\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            print(f\"Evaluation Metrics for {model_name}:\\nMean Squared Error: {mse:.4f}\\nRÂ² Score: {r2:.4f}\")\n",
    "\n",
    "    def plot_confusion_matrix(self, best_estimator, X_test, y_test):\n",
    "        \"\"\"Plot confusion matrix for classification.\"\"\"\n",
    "        y_pred = best_estimator.predict(X_test)\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "        disp.plot(cmap=plt.cm.Blues)\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        plt.show()\n",
    "\n",
    "    def plot_regression_results(self, best_estimator, X_test, y_test):\n",
    "        \"\"\"Plot regression results.\"\"\"\n",
    "        y_pred = best_estimator.predict(X_test)\n",
    "        plt.scatter(y_test, y_pred, alpha=0.7, edgecolors='k')\n",
    "        plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "        plt.xlabel(\"Actual\")\n",
    "        plt.ylabel(\"Predicted\")\n",
    "        plt.title(\"Regression Results\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Scoring criteria\n",
    "scoring_criteria = {\n",
    "    'classification': ['accuracy', 'f1_macro', 'balanced_accuracy', 'precision_macro', 'recall_macro', 'average_precision'],\n",
    "    'regression': [\n",
    "        'neg_mean_squared_error', 'r2', 'neg_mean_absolute_error', 'neg_median_absolute_error', \n",
    "        'max_error', 'explained_variance', 'neg_root_mean_squared_error'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Classifier and regressor parameter spaces\n",
    "models_param_spaces = {\n",
    "    'DecisionTreeClassifier': {\n",
    "        'bayesian': {\n",
    "            'model__criterion': Categorical(['gini', 'entropy', 'log_loss']),\n",
    "            'model__splitter': Categorical(['best', 'random']),\n",
    "            'model__max_depth': Integer(1, 1000),\n",
    "            'model__min_samples_split': Real(0.01, 0.9),\n",
    "            'model__min_samples_leaf': Real(0.01, 0.9),\n",
    "            'model__min_weight_fraction_leaf': Real(0.0,0.5),\n",
    "            'model__max_features': Real(0.01,0.9),\n",
    "            'model__max_leaf_nodes': Integer(2, 4000), \n",
    "            'model__min_impurity_decrease': Real(0.0, 1.0),\n",
    "            'model__ccp_alpha': Real(0.01, 0.9),\n",
    "            'pca__n_components': Integer(1, len(digits.data[0])),\n",
    "            'scaler__with_mean': [True, False],\n",
    "            'scaler__with_std': [True, False],\n",
    "        }\n",
    "    },\n",
    "    'RandomForestClassifier': {\n",
    "        'bayesian': {\n",
    "            'model__n_estimators': Integer(10, 2000),\n",
    "            'model__criterion': Categorical(['gini', 'entropy', 'log_loss']),\n",
    "            'model__max_depth': Integer(1, 1000), \n",
    "            'model__min_samples_split': Real(0.01, 0.9), \n",
    "            'model__min_samples_leaf': Real(0.01, 0.9),\n",
    "            'model__min_weight_fraction_leaf': Real(0.01,0.5),\n",
    "            '_model_max_features': Real(0.01,0.9),\n",
    "            'model__max_leaf_nodes': Integer(1,2000),\n",
    "            'model__min_impurity_decrease': Real(0.01,0.9),\n",
    "            # 'classify__bootstrap': Categorical([True, False]),\n",
    "            'model__oob_score': Categorical([True, False]),\n",
    "            'model__warm_start': Categorical([True, False]),\n",
    "            'model__max_samples':Real(0.01,0.9),\n",
    "            'pca__n_components': Integer(1, len(digits.data[0])),\n",
    "            'scaler__with_mean': [True, False],\n",
    "            'scaler__with_std': [True, False],\n",
    "        }\n",
    "    },\n",
    "    'DecisionTreeRegressor': {\n",
    "        'bayesian': {\n",
    "            'model__criterion': ['squared_error', 'friedman_mse', 'absolute_error', 'poisson'],\n",
    "            'model__splitter': ['best', 'random'],\n",
    "            'model__max_depth': Integer(2, 1000),\n",
    "            'model__min_samples_split': Real(0.01, 0.9),\n",
    "            'model__min_samples_leaf': Real(0.01, 0.9),\n",
    "            'model__min_weight_fraction_leaf': Real(0.0, 0.5),\n",
    "            'model__max_features': Real(0.01, 0.5),\n",
    "            'model__max_leaf_nodes': Integer(2, 1000),\n",
    "            'model__min_impurity_decrease': Real(0.0, 0.9),\n",
    "            'model__ccp_alpha': Real(0.01, 0.9),\n",
    "            'pca__n_components': Integer(1, len(modelData.columns)),\n",
    "            'scaler__with_mean': [True, False],\n",
    "            'scaler__with_std': [True, False],\n",
    "        }\n",
    "    },\n",
    "    'RandomForestRegressor': {\n",
    "        'bayesian': {\n",
    "            'model__n_estimators': Integer(50, 500),\n",
    "            'model__criterion': Categorical(['squared_error', 'friedman_mse', 'absolute_error', 'poisson']),\n",
    "            'model__max_depth': Integer(2, 1000), \n",
    "            'model__min_samples_split': Real(0.01, 0.9),\n",
    "            'model__min_samples_leaf': Real(0.01, 0.9),\n",
    "            'model__min_weight_fraction_leaf': Real(0.01, 0.5),\n",
    "            'model__max_features': Real(0.01,0.9),\n",
    "            # 'regress__max_features': Categorical(['sqrt', 'log2']), \n",
    "            'model__max_leaf_nodes': Integer(2,1000),\n",
    "            'model__min_impurity_decrease': Real(0.01, 0.9),\n",
    "            # 'regress__bootstrap': [True, False],\n",
    "            'model__oob_score': [True, False],\n",
    "            'model__warm_start': [True, False],\n",
    "            'model__ccp_alpha': Real(0.01, 0.9),\n",
    "            'model__max_samples': Real(0.01, 0.9),\n",
    "            'pca__n_components': Integer(1, len(modelData.columns)),\n",
    "            'scaler__with_mean': [True, False],\n",
    "            'scaler__with_std': [True, False],\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(digits.data, digits.target, test_size=0.3, random_state=42)\n",
    "\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(modelData, cleanTarget, test_size=0.3, random_state=42)\n",
    "\n",
    "tuner = ModelTuner(models_param_spaces, scoring_criteria)\n",
    "tuner.tune_all_models('classification', 'bayesian', X_train_clf, y_train_clf, X_test_clf, y_test_clf)\n",
    "tuner.tune_all_classifiers('regression', 'bayesian', X_train_reg, y_train_reg, X_test_reg, y_test_reg)\n",
    "\n",
    "# Evaluate a specific model\n",
    "tuner.evaluate_model('DecisionTreeClassifier', X_test, y_test)\n",
    "tuner.evaluate_model('RandomForestClassifier', X_test, y_test)\n",
    "tuner.evaluate_model('DecisionTreeRegressor', X_test, y_test)\n",
    "tuner.evaluate_model('RandomForestRegressor', X_test, y_test)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
