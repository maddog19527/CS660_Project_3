{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Install non-standard libraries'''\n",
    "!pip install skopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Generic Imports'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from scipy.stats import randint, uniform, zscore\n",
    "\n",
    "'''Visualization Imports'''\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "'''Import Data'''\n",
    "from sklearn.datasets import load_digits, fetch_california_housing\n",
    "\n",
    "'''Import Data Processing Utilities'''\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "'''Import Predictors'''\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "\n",
    "'''Import Model Tuning Utilities'''\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "\n",
    "'''Import Metrics'''\n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    confusion_matrix, \n",
    "    mean_squared_error, \n",
    "    mean_absolute_error, \n",
    "    r2_score, \n",
    "    explained_variance_score, \n",
    "    max_error, \n",
    "    mean_squared_log_error, \n",
    "    median_absolute_error, \n",
    "    mean_absolute_percentage_error, \n",
    "    mean_poisson_deviance,\n",
    "    mean_gamma_deviance\n",
    ")\n",
    "\n",
    "'''Suppress Warnings'''\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Import Classifier data'''\n",
    "digits = load_digits()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Instantiate and Evaluate Default Classifiers'''\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.3, random_state=42)\n",
    "for model in [DecisionTreeClassifier, RandomForestClassifier]:\n",
    "    pipe = Pipeline([\n",
    "        ('scale', StandardScaler()),\n",
    "        # ('pca', PCA()), PCA Unused due to negative impact on model performance\n",
    "        ('classify',model())],\n",
    "    verbose=True)\n",
    "    predicted = pipe.fit(X_train, y_train).predict(X_test)\n",
    "    print(classification_report(y_test, predicted))\n",
    "    sns.heatmap(confusion_matrix(y_test, predicted))\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Instantiate and Tune Classifiers'''\n",
    "\n",
    "classifiers = {\n",
    "    DecisionTreeClassifier.__name__: {\n",
    "        'model': DecisionTreeClassifier,\n",
    "        'paramSpace': {\n",
    "            'classify__criterion': Categorical(['gini', 'entropy', 'log_loss']),\n",
    "            'classify__splitter': Categorical(['best', 'random']),\n",
    "            'classify__max_depth': Integer(1, 1000),\n",
    "            'classify__min_samples_split': Real(0.01, 0.9),\n",
    "            'classify__min_samples_leaf': Real(0.01, 0.9),\n",
    "            'classify__min_weight_fraction_leaf': Real(0.0,0.5),\n",
    "            'classify__max_features': Real(0.01,0.9),\n",
    "            'classify__max_leaf_nodes': Integer(2, 4000), \n",
    "            'classify__min_impurity_decrease': Real(0.0, 1.0),\n",
    "            'classify__ccp_alpha': Real(0.01, 0.9),\n",
    "            'pca__n_components': Integer(1,len(digits.data[0])),\n",
    "            'scaler__with_mean': [True, False],\n",
    "            'scaler__with_std': [True, False],\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    RandomForestClassifier.__name__: {\n",
    "        'model': RandomForestClassifier,\n",
    "        'paramSpace': {\n",
    "            'classify__n_estimators': Integer(10, 2000),\n",
    "            'classify__criterion': Categorical(['gini', 'entropy', 'log_loss']),\n",
    "            'classify__max_depth': Integer(1, 1000), \n",
    "            'classify__min_samples_split': Real(0.01, 0.9), \n",
    "            'classify__min_samples_leaf': Real(0.01, 0.9),\n",
    "            'classify__min_weight_fraction_leaf': Real(0.01,0.5),\n",
    "            'classify__max_features': Real(0.01,0.9),\n",
    "            'classify__max_leaf_nodes': Integer(1,2000),\n",
    "            'classify__min_impurity_decrease': Real(0.01,0.9),\n",
    "            # 'classify__bootstrap': Categorical([True, False]),\n",
    "            'classify__oob_score': Categorical([True, False]),\n",
    "            'classify__warm_start': Categorical([True, False]),\n",
    "            'classify__max_samples':Real(0.01,0.9),\n",
    "            'pca__n_components': Integer(1,len(digits.data[0])),\n",
    "            'scaler__with_mean': [True, False],\n",
    "            'scaler__with_std': [True, False],\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "tunedModels = {}\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.3, random_state=42)\n",
    "for name, classDict in classifiers.items():\n",
    "    \n",
    "    pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('pca', PCA()),\n",
    "        ('classify',classDict['model']())],\n",
    "    verbose=False)\n",
    "\n",
    "    tunedModels[name] = BayesSearchCV(\n",
    "        pipe,\n",
    "        classDict['paramSpace'],\n",
    "        n_iter= 5,#30, # Reduced for performance during development\n",
    "        cv= 4,#20, # Reduced for performance during development\n",
    "        scoring='accuracy',\n",
    "        # TODO: Use GridSearch for scoring criteria\n",
    "        # NOTE: This will take 13 hours. Execute over night.\n",
    "        random_state=42\n",
    "        )\n",
    "    tunedModels[name].fit(X_train, y_train)\n",
    "    \n",
    "    # search = BayesSearchCV(pipe, param_grid, n_jobs=2)\n",
    "    # predicted = search.best_estimator_.predict(X_test)\n",
    "    predicted = tunedModels[name].best_estimator_.predict(X_test)\n",
    "    \n",
    "    print(classification_report(y_test, predicted))\n",
    "    sns.heatmap(confusion_matrix(y_test, predicted))\n",
    "    plt.title(name + ' Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Import Regression Data'''\n",
    "cal_housing = fetch_california_housing(as_frame=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Data Cleaning and Pre-Processing'''\n",
    "cleanData = cal_housing.data\n",
    "cleanData['y'] = cal_housing.target\n",
    "\n",
    "cleanData = cleanData.drop(columns=['Longitude', 'Latitude'])\n",
    "for feature in ['AveBedrms', 'AveRooms', 'AveOccup', 'Population']:\n",
    "    cleanData = cleanData[(np.abs(zscore(cleanData[feature])) < 2.5)]\n",
    "cleanTarget = cleanData['y'].to_list()\n",
    "\n",
    "# DATA CLEANING TODOs\n",
    "# TODO: Bin Lat/Long groupings into city/town clusters. look for available geo-fencing data for cluster labeling - can we do a graph of centroids on top of map?\n",
    "# TODO: Fix Skew for Population, MedIncome, AvgOccup, AvgBedroom, Target\n",
    "# TODO: Feature Engineering / Reduction\n",
    "cleanData.drop(columns=['y'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Instantiate and Evaluate Default Regressors'''\n",
    "modelData = cleanData.copy()\n",
    "\n",
    "# TODO: How does normalization vs standardization impact model performance\n",
    "transformPipeline = [\n",
    "    ('scaler', StandardScaler()),\n",
    "    # ('feature_reduction', PCA(n_components=5,iterated_power=7))\n",
    "    ]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(modelData, cleanTarget, test_size=0.3, random_state=42)\n",
    "for regressor in [DecisionTreeRegressor, RandomForestRegressor]:\n",
    "    pipe = Pipeline(transformPipeline + [('regress',regressor())], verbose=True)\n",
    "    predicted = pipe.fit(X_train, y_train).predict(X_test)\n",
    "    mse = mean_squared_error(y_test, predicted)\n",
    "    print(regressor.__name__)\n",
    "    print(mse)\n",
    "    \n",
    "    \n",
    "    # TODO: Graph Regression Plane using skopt.plots\n",
    "    # NOTE: try using PCA to force data into 3d space\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Instantiate and Evaluate Default Regressors'''\n",
    "modelData = cleanData.copy()\n",
    "\n",
    "# TODO: How does normalization vs standardization impact model performance\n",
    "transformPipeline = [\n",
    "    ('scaler', MinMaxScaler()),\n",
    "    # ('feature_reduction', PCA(n_components=5,iterated_power=7))\n",
    "    ]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(modelData, cleanTarget, test_size=0.3, random_state=42)\n",
    "for regressor in [DecisionTreeRegressor, RandomForestRegressor]:\n",
    "    pipe = Pipeline(transformPipeline + [('regress',regressor())], verbose=True)\n",
    "    predicted = pipe.fit(X_train, y_train).predict(X_test)\n",
    "    mse = mean_squared_error(y_test, predicted)\n",
    "    print(regressor.__name__)\n",
    "    print(mse)\n",
    "    \n",
    "    \n",
    "    # TODO: Graph Regression Plane using skopt.plots\n",
    "    # NOTE: try using PCA to force data into 3d space\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Instantiate and Tune Regressors'''\n",
    "# TODO: Explore how increased demetionality in the parameter space impacts optimization performance\n",
    "regressors = {\n",
    "    DecisionTreeRegressor.__name__: {\n",
    "        'model': DecisionTreeRegressor,\n",
    "        'paramSpace': {\n",
    "            'regress__criterion': ['squared_error', 'friedman_mse', 'absolute_error', 'poisson'],\n",
    "            'regress__splitter': ['best', 'random'],\n",
    "            'regress__max_depth': Integer(2, 1000),\n",
    "            'regress__min_samples_split': Real(0.01, 0.9),\n",
    "            'regress__min_samples_leaf': Real(0.01, 0.9),\n",
    "            'regress__min_weight_fraction_leaf': Real(0.0, 0.5),\n",
    "            'regress__max_features': Real(0.01, 0.5),\n",
    "            'regress__max_leaf_nodes': Integer(2, 1000),\n",
    "            'regress__min_impurity_decrease': Real(0.0, 0.9),\n",
    "            'regress__ccp_alpha': Real(0.01, 0.9),\n",
    "            'pca__n_components': Integer(1,len(modelData.columns)),\n",
    "            'scaler__with_mean': [True, False],\n",
    "            'scaler__with_std': [True, False],\n",
    "        }\n",
    "    },\n",
    "    RandomForestRegressor.__name__: {\n",
    "        'model': RandomForestRegressor,\n",
    "        'paramSpace': {\n",
    "            'regress__n_estimators': Integer(50, 500),\n",
    "            'regress__criterion': Categorical(['squared_error', 'friedman_mse', 'absolute_error', 'poisson']),\n",
    "            'regress__max_depth': Integer(2, 1000), \n",
    "            'regress__min_samples_split': Real(0.01, 0.9),\n",
    "            'regress__min_samples_leaf': Real(0.01, 0.9),\n",
    "            'regress__min_weight_fraction_leaf': Real(0.01, 0.5),\n",
    "            'regress__max_features': Real(0.01,0.9),\n",
    "            # 'regress__max_features': Categorical(['sqrt', 'log2']), \n",
    "            'regress__max_leaf_nodes': Integer(2,1000),\n",
    "            'regress__min_impurity_decrease': Real(0.01, 0.9),\n",
    "            # 'regress__bootstrap': [True, False],\n",
    "            'regress__oob_score': [True, False],\n",
    "            'regress__warm_start': [True, False],\n",
    "            'regress__ccp_alpha': Real(0.01, 0.9),\n",
    "            'regress__max_samples': Real(0.01, 0.9),\n",
    "            'pca__n_components': Integer(1,len(modelData.columns)),\n",
    "            'scaler__with_mean': [True, False],\n",
    "            'scaler__with_std': [True, False],\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "tunedModel = {}\n",
    "X_train, X_test, y_train, y_test = train_test_split(modelData, cleanTarget, test_size=0.3, random_state=42)\n",
    "\n",
    "for name, regDict in regressors.items():\n",
    "    pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('pca', PCA()),\n",
    "        ('regress',regDict['model']())])\n",
    "    tunedModel[name] = BayesSearchCV(\n",
    "        pipe,\n",
    "        regDict['paramSpace'],\n",
    "        n_iter=5,#30,\n",
    "        cv=5,#20,\n",
    "        # scoring=scoreModel\n",
    "        # scoring = scoringCriteria[i]\n",
    "        # TODO: Use GridSearchCV for scoringCriteria param space\n",
    "        # NOTE: This will take 13 hours. Execute over night.\n",
    "        )\n",
    "    tunedModel[name].fit(X_train, y_train)\n",
    "    predicted = tunedModel[name].best_estimator_.predict(X_test)\n",
    "    print('Evaluation Metric:', tunedModel[name].get_params()['scoring'])\n",
    "    print(\"val. score: %s\" % tunedModel[name].best_score_)\n",
    "    print(\"test score: %s\" % tunedModel[name].score(X_test, y_test))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEW CODE BELOW!! - Will"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalers = {'Standardization': StandardScaler(), 'Normalization': MinMaxScaler()}\n",
    "X_train, X_test, y_train, y_test = train_test_split(modelData, cleanTarget, test_size=0.3, random_state=42)\n",
    "\n",
    "for scale_name, scaler in scalers.items():\n",
    "    print(f\"Testing with {scale_name}\")\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    for regressor in [DecisionTreeRegressor, RandomForestRegressor]:\n",
    "        # Define and fit pipeline\n",
    "        pipe = Pipeline([('scaler', scaler), ('regress', regressor())], verbose=True)\n",
    "        predicted = pipe.fit(X_train, y_train).predict(X_test)\n",
    "        y_test_np = np.array(y_test)\n",
    "        \n",
    "        # Plot Actual vs Predicted\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.scatter(y_test, predicted, alpha=0.7, color=\"blue\", label=\"Predicted vs Actual\")\n",
    "        plt.plot([y_test_np.min(), y_test_np.max()], [y_test_np.min(), y_test_np.max()], color=\"red\", lw=2, label=\"Ideal\")\n",
    "        plt.title(f\"Actual vs Predicted - {regressor.__name__}\")\n",
    "        plt.xlabel(\"Actual\")\n",
    "        plt.ylabel(\"Predicted\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "        # Plot Residuals\n",
    "        residuals = y_test - predicted\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.scatter(predicted, residuals, alpha=0.7, color=\"purple\")\n",
    "        plt.axhline(0, color=\"red\", linestyle=\"--\")\n",
    "        plt.title(f\"Residuals - {regressor.__name__}\")\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"Residuals\")\n",
    "        plt.show()\n",
    "\n",
    "        # Apply PCA for 3D Plot\n",
    "        pca = PCA(n_components=2)\n",
    "        X_train_pca = pca.fit_transform(X_train)\n",
    "        X_test_pca = pca.transform(X_test)\n",
    "        \n",
    "        fig = plt.figure(figsize=(10, 7))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        ax.scatter(X_test_pca[:, 0], X_test_pca[:, 1], y_test_np, color='blue', label='Actual')\n",
    "        ax.scatter(X_test_pca[:, 0], X_test_pca[:, 1], predicted, color='green', alpha=0.7, label='Predicted')\n",
    "        ax.set_title(\"3D PCA Regression Plot\")\n",
    "        ax.set_xlabel(\"Principal Component 1\")\n",
    "        ax.set_ylabel(\"Principal Component 2\")\n",
    "        ax.set_zlabel(\"Output\")\n",
    "        ax.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # Calculate and print metrics\n",
    "        print('Mean Absolute Error (MAE):', mean_absolute_error(y_test, predicted))\n",
    "        print('Mean Squared Error (MSE):', mean_squared_error(y_test, predicted))                           # preferred metric\n",
    "        print('Root Mean Squared Error (RMSE):', np.sqrt(mean_squared_error(y_test, predicted)))\n",
    "        print('Mean Absolute Percentage Error (MAPE):', mean_absolute_percentage_error(y_test, predicted))\n",
    "        print('Explained Variance Score:', explained_variance_score(y_test, predicted))\n",
    "        print('Max Error:', max_error(y_test, predicted))\n",
    "        print('Mean Squared Log Error:', mean_squared_log_error(y_test, predicted))\n",
    "        print('Median Absolute Error:', median_absolute_error(y_test, predicted))\n",
    "        print('R^2:', r2_score(y_test, predicted))                                                          # preferred metric\n",
    "        print('Mean Poisson Deviance:', mean_poisson_deviance(y_test, predicted))\n",
    "        print('Mean Gamma Deviance:', mean_gamma_deviance(y_test, predicted))\n",
    "        print('-' * 50)  # Separator between different model results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Metrics\n",
    "classification_metrics = [\n",
    "    'accuracy', 'f1_macro', 'roc_auc', 'balanced_accuracy',\n",
    "    'precision_macro', 'recall_macro', 'average_precision'\n",
    "]\n",
    "\n",
    "# Filter only regression-compatible scoring metrics\n",
    "regression_scoring_criteria = [\n",
    "    'neg_mean_squared_error', 'r2', 'neg_mean_absolute_error',\n",
    "    'neg_mean_squared_log_error', 'neg_median_absolute_error',\n",
    "    'max_error', 'explained_variance', 'neg_root_mean_squared_error'\n",
    "]\n",
    "\n",
    "\n",
    "# Define scoring criteria\n",
    "scoring_criteria = {\n",
    "    'classification': ['accuracy', 'f1_macro', 'roc_auc', 'balanced_accuracy', 'precision_macro', 'recall_macro', 'average_precision'],\n",
    "    'regression': [\n",
    "        'neg_mean_squared_error', 'r2', 'neg_mean_absolute_error', 'neg_mean_squared_log_error', \n",
    "        'neg_median_absolute_error', 'max_error', 'explained_variance', 'neg_root_mean_squared_error'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Regressors parameter spaces for GridSearchCV and RandomizedSearchCV\n",
    "param_spaces = {\n",
    "    'DecisionTreeRegressor': {\n",
    "        'randomized': {\n",
    "            'regress__criterion': ['squared_error', 'friedman_mse', 'absolute_error', 'poisson'],\n",
    "            'regress__splitter': ['best', 'random'],\n",
    "            'regress__max_depth': randint(2, 101),\n",
    "            'regress__min_samples_split': uniform(0.01, 0.89),\n",
    "            'regress__min_samples_leaf': uniform(0.01, 0.89),\n",
    "            'regress__max_features': uniform(0.01, 0.49),\n",
    "            'regress__ccp_alpha': uniform(0.01, 0.89),\n",
    "            'pca__n_components': randint(1, 100),\n",
    "            'scaler__with_mean': [True, False],\n",
    "            'scaler__with_std': [True, False]\n",
    "        },\n",
    "        'grid': {\n",
    "            'regress__splitter': ['best', 'random'],\n",
    "            'regress__max_depth': list(range(2, 1001)),\n",
    "            'regress__min_samples_split': [i / 100 for i in range(1, 91)],\n",
    "            'regress__min_samples_leaf': [i / 100 for i in range(1, 91)],\n",
    "            'regress__max_features': [i / 100 for i in range(1, 51)],\n",
    "            'regress__ccp_alpha': [i / 100 for i in range(1, 91)],\n",
    "            'pca__n_components': list(range(1, 101)),\n",
    "            'scaler__with_mean': [True, False],\n",
    "            'scaler__with_std': [True, False]\n",
    "        }\n",
    "    },\n",
    "    'RandomForestRegressor': {\n",
    "        'randomized': {\n",
    "            'regress__n_estimators': randint(50, 501),\n",
    "            'regress__max_depth': randint(2, 101),\n",
    "            'regress__min_samples_split': uniform(0.01, 0.89),\n",
    "            'regress__min_samples_leaf': uniform(0.01, 0.89),\n",
    "            'regress__max_features': uniform(0.01, 0.89),\n",
    "            'regress__ccp_alpha': uniform(0.01, 0.89),\n",
    "            'regress__max_samples': uniform(0.01, 0.89),\n",
    "            'pca__n_components': randint(1, 100),\n",
    "            'scaler__with_mean': [True, False],\n",
    "            'scaler__with_std': [True, False]\n",
    "        },\n",
    "        'grid': {\n",
    "            'regress__n_estimators': list(range(50, 501)),\n",
    "            'regress__max_depth': list(range(2, 1001)),\n",
    "            'regress__min_samples_split': [i / 100 for i in range(1, 91)],\n",
    "            'regress__min_samples_leaf': [i / 100 for i in range(1, 91)],\n",
    "            'regress__max_features': [i / 100 for i in range(1, 91)],\n",
    "            'regress__ccp_alpha': [i / 100 for i in range(1, 91)],\n",
    "            'regress__max_samples': [i / 100 for i in range(1, 91)],\n",
    "            'pca__n_components': list(range(1, 101)),\n",
    "            'scaler__with_mean': [True, False],\n",
    "            'scaler__with_std': [True, False]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Function to perform GridSearchCV or RandomizedSearchCV\n",
    "def tune_model(search_type, model_name, model, param_space, X_train, y_train, X_test, y_test):\n",
    "    pipe = Pipeline([ \n",
    "        ('scaler', StandardScaler()), \n",
    "        ('pca', PCA()), \n",
    "        ('regress', model()) \n",
    "    ])\n",
    "\n",
    "    # Set search class based on the search_type\n",
    "    search_class = RandomizedSearchCV if search_type == 'randomized' else GridSearchCV\n",
    "\n",
    "    # Set parameters based on search type\n",
    "    search_params = {\n",
    "        'estimator': pipe,\n",
    "        'n_jobs': -1,\n",
    "        'cv': 5,\n",
    "        'scoring': None,\n",
    "        'return_train_score': True\n",
    "    }\n",
    "\n",
    "    # For RandomizedSearchCV, use 'param_distributions' and set 'n_iter'\n",
    "    if search_type == 'randomized':\n",
    "        search_params['n_iter'] = 50  # Only for RandomizedSearchCV\n",
    "        search_params['param_distributions'] = param_space  # Use for RandomizedSearchCV\n",
    "    else:\n",
    "        search_params['param_grid'] = param_space  # Use param_grid for GridSearchCV\n",
    "\n",
    "    search = search_class(**search_params)\n",
    "\n",
    "    # Loop over scoring metrics for evaluation\n",
    "    for scoring_metric in scoring_criteria['regression']:\n",
    "        print(f'Tuning {model_name} with {search_type} search and scoring metric: {scoring_metric}')\n",
    "        search.set_params(scoring=scoring_metric)\n",
    "        search.fit(X_train, y_train)\n",
    "        \n",
    "        print(f'Evaluation Metric: {scoring_metric}')\n",
    "        print(f\"Best Score (Train): {search.best_score_}\")\n",
    "        print(f\"Test Score: {search.score(X_test, y_test)}\")\n",
    "        print('-' * 80)\n",
    "\n",
    "    return search\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(modelData, cleanTarget, test_size=0.3, random_state=42)\n",
    "\n",
    "# Instantiate tuned models dictionary\n",
    "tuned_models = {}\n",
    "\n",
    "# Iterate through each regressor and perform tuning\n",
    "for model_name, model_dict in param_spaces.items():\n",
    "    # Run RandomizedSearchCV first\n",
    "    if 'randomized' in model_dict:\n",
    "        param_space = model_dict['randomized']\n",
    "        search = tune_model('randomized', model_name, globals()[model_name], param_space, X_train, y_train, X_test, y_test)\n",
    "        tuned_models[model_name] = search\n",
    "\n",
    "    # Then run GridSearchCV\n",
    "    if 'grid' in model_dict:\n",
    "        param_space = model_dict['grid']\n",
    "        search = tune_model('grid', model_name, globals()[model_name], param_space, X_train, y_train, X_test, y_test)\n",
    "        tuned_models[model_name] = search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = opt.best_estimator_.steps[-1][1]\n",
    "features = modelData.columns\n",
    "importance = regressor.feature_importances_\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(list(modelData.columns), importance)\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Evaluate tuning process and resultant models'''\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
