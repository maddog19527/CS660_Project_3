{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "> This is the workbook that we will submit.\n",
    "> All relevant visuals and data analysis for our report should be produced by executing this workbook's cells in order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Install non-standard libraries'''\n",
    "!pip install skopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Generic Imports'''\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "'''Visualization Imports'''\n",
    "from prettytable import PrettyTable\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "'''Import Data'''\n",
    "from sklearn.datasets import load_digits, fetch_california_housing\n",
    "\n",
    "'''Import Data Processing Utilities'''\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, mean_squared_error\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "'''Import Predictors'''\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "\n",
    "'''Import Model Tuning Utilities'''\n",
    "from skopt import BayesSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Define Global Variables'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Import Classifier data'''\n",
    "digits = load_digits()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Data Visualization and EDA'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Instantiate and Evaluate Default Classifiers'''\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.3, random_state=42)\n",
    "for model in [DecisionTreeClassifier, RandomForestClassifier]:\n",
    "    pipe = Pipeline([\n",
    "        ('scale', StandardScaler()),\n",
    "        # ('pca', PCA()), PCA Unused due to negative impact on model performance\n",
    "        ('classify',model())],\n",
    "    verbose=True)\n",
    "    predicted = pipe.fit(X_train, y_train).predict(X_test)\n",
    "    print(classification_report(y_test, predicted))\n",
    "    sns.heatmap(confusion_matrix(y_test, predicted))\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Instantiate and Tune Classifiers'''\n",
    "\n",
    "classifiers = {\n",
    "    DecisionTreeClassifier.__name__: {\n",
    "        'model': DecisionTreeClassifier,\n",
    "        'paramSpace': {\n",
    "            'classify__criterion': Categorical(['gini', 'entropy', 'log_loss']),\n",
    "            'classify__splitter': Categorical(['best', 'random']),\n",
    "            'classify__max_depth': Integer(1, 1000),\n",
    "            'classify__min_samples_split': Real(0.01, 0.9),\n",
    "            'classify__min_samples_leaf': Real(0.01, 0.9),\n",
    "            'classify__min_weight_fraction_leaf': Real(0.0,0.5),\n",
    "            'classify__max_features': Real(0.01,0.9),\n",
    "            'classify__max_leaf_nodes': Integer(2, 4000), \n",
    "            'classify__min_impurity_decrease': Real(0.0, 1.0),\n",
    "            'classify__ccp_alpha': Real(0.01, 0.9),\n",
    "            'pca__n_components': Integer(1,len(digits.data[0])),\n",
    "            'scaler__with_mean': [True, False],\n",
    "            'scaler__with_std': [True, False],\n",
    "        }\n",
    "    },\n",
    "    RandomForestClassifier.__name__: {\n",
    "        'model': RandomForestClassifier,\n",
    "        'paramSpace': {\n",
    "            'classify__n_estimators': Integer(10, 2000),\n",
    "            'classify__criterion': Categorical(['gini', 'entropy', 'log_loss']),\n",
    "            'classify__max_depth': Integer(1, 1000), \n",
    "            'classify__min_samples_split': Real(0.01, 0.9), \n",
    "            'classify__min_samples_leaf': Real(0.01, 0.9),\n",
    "            'classify__min_weight_fraction_leaf': Real(0.01,0.5),\n",
    "            'classify__max_features': Real(0.01,0.9),\n",
    "            'classify__max_leaf_nodes': Integer(1,2000),\n",
    "            'classify__min_impurity_decrease': Real(0.01,0.9),\n",
    "            # 'classify__bootstrap': Categorical([True, False]),\n",
    "            'classify__oob_score': Categorical([True, False]),\n",
    "            'classify__warm_start': Categorical([True, False]),\n",
    "            'classify__max_samples':Real(0.01,0.9),\n",
    "            'pca__n_components': Integer(1,len(digits.data[0])),\n",
    "            'scaler__with_mean': [True, False],\n",
    "            'scaler__with_std': [True, False],\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "tunedModels = {}\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.3, random_state=42)\n",
    "for name, classDict in classifiers.items():\n",
    "    \n",
    "    pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('pca', PCA()),\n",
    "        ('classify',classDict['model']())],\n",
    "    verbose=False)\n",
    "\n",
    "    tunedModels[name] = BayesSearchCV(\n",
    "        pipe,\n",
    "        classDict['paramSpace'],\n",
    "        n_iter= 5,#30, # Reduced for performance during development\n",
    "        cv= 4,#20, # Reduced for performance during development\n",
    "        scoring='accuracy',\n",
    "        # TODO: Use GridSearch for scoring criteria\n",
    "        # NOTE: This will take 13 hours. Execute over night.\n",
    "        random_state=42\n",
    "        )\n",
    "    tunedModels[name].fit(X_train, y_train)\n",
    "    \n",
    "    # search = BayesSearchCV(pipe, param_grid, n_jobs=2)\n",
    "    # predicted = search.best_estimator_.predict(X_test)\n",
    "    predicted = tunedModels[name].best_estimator_.predict(X_test)\n",
    "    \n",
    "    print(classification_report(y_test, predicted))\n",
    "    sns.heatmap(confusion_matrix(y_test, predicted))\n",
    "    plt.title(name + ' Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Evaluate tuning process and resultant models'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Import Regression Data'''\n",
    "cal_housing = fetch_california_housing(as_frame=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Initial data EDA'''\n",
    "\n",
    "outlierTable = PrettyTable(['Feature', 'Outlier Count'])\n",
    "\n",
    "for column in cal_housing.data.columns:\n",
    "    Q1 = cal_housing.data[column].quantile(0.25)\n",
    "    Q3 = cal_housing.data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - 1.5*IQR\n",
    "    upper = Q3 + 1.5*IQR\n",
    "    outlierCount = np.array(cal_housing.data[column] >= upper).sum() + np.array(cal_housing.data[column] <= lower).sum()\n",
    "    outlierTable.add_row([column, outlierCount])\n",
    "    # print(f\"{column}: {outlierCount}\")\n",
    "\n",
    "print(outlierTable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Data Cleaning and Pre-Processing'''\n",
    "cleanData = cal_housing.data\n",
    "cleanData['y'] = cal_housing.target\n",
    "\n",
    "cleanData = cleanData.drop(columns=['Longitude', 'Latitude'])\n",
    "for feature in ['AveBedrms', 'AveRooms', 'AveOccup', 'Population']:\n",
    "    cleanData = cleanData[(np.abs(stats.zscore(cleanData[feature])) < 2.5)]\n",
    "cleanTarget = cleanData['y'].to_list()\n",
    "\n",
    "# DATA CLEANING TODOs\n",
    "# TODO: Bin Lat/Long groupings into city/town clusters. look for available geo-fencing data for cluster labeling - can we do a graph of centroids on top of map?\n",
    "# TODO: Fix Skew for Population, MedIncome, AvgOccup, AvgBedroom, Target\n",
    "# TODO: Feature Engineering / Reduction\n",
    "cleanData.drop(columns=['y'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Instantiate and Evaluate Default Regressors'''\n",
    "modelData = cleanData.copy()\n",
    "\n",
    "# TODO: How does normalization vs standardization impact model performance\n",
    "transformPipeline = [\n",
    "    ('scaler', StandardScaler()),\n",
    "    # ('feature_reduction', PCA(n_components=5,iterated_power=7))\n",
    "    ]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(modelData, cleanTarget, test_size=0.3, random_state=42)\n",
    "for regressor in [DecisionTreeRegressor, RandomForestRegressor]:\n",
    "    pipe = Pipeline(transformPipeline + [('regress',regressor())], verbose=True)\n",
    "    predicted = pipe.fit(X_train, y_train).predict(X_test)\n",
    "    mse = mean_squared_error(y_test, predicted)\n",
    "    print(regressor.__name__)\n",
    "    print(mse)\n",
    "    \n",
    "    \n",
    "    # TODO: Graph Regression Plane using skopt.plots\n",
    "    # NOTE: try using PCA to force data into 3d space\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Instantiate and Tune Regressors'''\n",
    "# TODO: Explore how increased demetionality in the parameter space impacts optimization performance\n",
    "regressors = {\n",
    "    DecisionTreeRegressor.__name__: {\n",
    "        'model': DecisionTreeRegressor,\n",
    "        'paramSpace': {\n",
    "            'regress__criterion': ['squared_error', 'friedman_mse', 'absolute_error', 'poisson'],\n",
    "            'regress__splitter': ['best', 'random'],\n",
    "            'regress__max_depth': Integer(2, 1000),\n",
    "            'regress__min_samples_split': Real(0.01, 0.9),\n",
    "            'regress__min_samples_leaf': Real(0.01, 0.9),\n",
    "            'regress__min_weight_fraction_leaf': Real(0.0, 0.5),\n",
    "            'regress__max_features': Real(0.01, 0.5),\n",
    "            'regress__max_leaf_nodes': Integer(2, 1000),\n",
    "            'regress__min_impurity_decrease': Real(0.0, 0.9),\n",
    "            'regress__ccp_alpha': Real(0.01, 0.9),\n",
    "            'pca__n_components': Integer(1,len(modelData.columns)),\n",
    "            'scaler__with_mean': [True, False],\n",
    "            'scaler__with_std': [True, False],\n",
    "        }\n",
    "    },\n",
    "    RandomForestRegressor.__name__: {\n",
    "        'model': RandomForestRegressor,\n",
    "        'paramSpace': {\n",
    "            'regress__n_estimators': Integer(50, 500),\n",
    "            'regress__criterion': Categorical(['squared_error', 'friedman_mse', 'absolute_error', 'poisson']),\n",
    "            'regress__max_depth': Integer(2, 1000), \n",
    "            'regress__min_samples_split': Real(0.01, 0.9),\n",
    "            'regress__min_samples_leaf': Real(0.01, 0.9),\n",
    "            'regress__min_weight_fraction_leaf': Real(0.01, 0.5),\n",
    "            'regress__max_features': Real(0.01,0.9),\n",
    "            # 'regress__max_features': Categorical(['sqrt', 'log2']), \n",
    "            'regress__max_leaf_nodes': Integer(2,1000),\n",
    "            'regress__min_impurity_decrease': Real(0.01, 0.9),\n",
    "            # 'regress__bootstrap': [True, False],\n",
    "            'regress__oob_score': [True, False],\n",
    "            'regress__warm_start': [True, False],\n",
    "            'regress__ccp_alpha': Real(0.01, 0.9),\n",
    "            'regress__max_samples': Real(0.01, 0.9),\n",
    "            'pca__n_components': Integer(1,len(modelData.columns)),\n",
    "            'scaler__with_mean': [True, False],\n",
    "            'scaler__with_std': [True, False],\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "tunedModel = {}\n",
    "X_train, X_test, y_train, y_test = train_test_split(modelData, cleanTarget, test_size=0.3, random_state=42)\n",
    "\n",
    "for name, regDict in regressors.items():\n",
    "    pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('pca', PCA()),\n",
    "        ('regress',regDict['model']())])\n",
    "    tunedModel[name] = BayesSearchCV(\n",
    "        pipe,\n",
    "        regDict['paramSpace'],\n",
    "        n_iter=5,#30,\n",
    "        cv=5,#20,\n",
    "        # scoring=scoreModel\n",
    "        # scoring = scoringCriteria[i]\n",
    "        # TODO: Use GridSearchCV for scoringCriteria param space\n",
    "        # NOTE: This will take 13 hours. Execute over night.\n",
    "        )\n",
    "    tunedModel[name].fit(X_train, y_train)\n",
    "    predicted = tunedModel[name].best_estimator_.predict(X_test)\n",
    "    print('Evaluation Metric:', tunedModel[name].get_params()['scoring'])\n",
    "    print(\"val. score: %s\" % tunedModel[name].best_score_)\n",
    "    print(\"test score: %s\" % tunedModel[name].score(X_test, y_test))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Evaluate tuning process and resultant models'''\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
