{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "> This is the workbook that we will submit.\n",
    "> All relevant visuals and data analysis for our report should be produced by executing this workbook's cells in order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Install non-standard libraries'''\n",
    "!pip install skopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Generic Imports'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from scipy.stats import randint, uniform, zscore\n",
    "\n",
    "'''Visualization Imports'''\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "'''Import Data'''\n",
    "from sklearn.datasets import load_digits, fetch_california_housing\n",
    "\n",
    "'''Import Data Processing Utilities'''\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "'''Import Predictors'''\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "\n",
    "'''Import Model Tuning Utilities'''\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "\n",
    "'''Import Metrics'''\n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    confusion_matrix, \n",
    "    mean_squared_error, \n",
    "    mean_absolute_error, \n",
    "    r2_score, \n",
    "    explained_variance_score, \n",
    "    max_error, \n",
    "    mean_squared_log_error, \n",
    "    median_absolute_error, \n",
    "    mean_absolute_percentage_error, \n",
    "    mean_poisson_deviance,\n",
    "    mean_gamma_deviance\n",
    ")\n",
    "\n",
    "'''Suppress Warnings'''\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Define Global Variables'''\n",
    "\n",
    "'''Define Helper Functions'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Import Classifier data'''\n",
    "digits = load_digits()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Data Visualization and EDA'''\n",
    "\n",
    "print('--- DIGITS EDA ---')\n",
    "\n",
    "# Load digits dataset\n",
    "digits = load_digits()\n",
    "\n",
    "# Convert to DataFrame for easier manipulation\n",
    "df_digits = pd.DataFrame(digits.data, columns=[f'Pixel_{i}' for i in range(digits.data.shape[1])])\n",
    "df_digits['Target'] = digits.target\n",
    "\n",
    "print(f'\\nMissing Values: {df_digits.isna().sum().sum()}'\n",
    "      f'\\nDuplicates: {df_digits.duplicated().sum()}'\n",
    "      f'\\nShape: {df_digits.shape}')\n",
    "\n",
    "print(df_digits.info())\n",
    "\n",
    "df_digits.describe()\n",
    "\n",
    "'''# Distribution of Target Digits\n",
    "target_counts = df_digits['Target'].value_counts()\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(x=target_counts.index, y=target_counts.values, palette='viridis')\n",
    "plt.title(\"Distribution of Target Digits\")\n",
    "plt.xlabel(\"Digits\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()'''\n",
    "\n",
    "# Correlation Heatmap (First 20 Pixels)\n",
    "correlation_matrix = df_digits.corr()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix.iloc[:20, :20], cmap='coolwarm', cbar=True, vmin=-1, vmax=1)\n",
    "plt.title(\"Correlation Heatmap (First 20 Pixels)\")\n",
    "plt.show()\n",
    "'''\n",
    "# Feature Variance and Low-Variance Features\n",
    "feature_variances = df_digits.var().drop('Target')\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.lineplot(x=range(len(feature_variances)), y=feature_variances, color='blue')\n",
    "plt.title(\"Feature Variances\")\n",
    "plt.xlabel(\"Feature Index\")\n",
    "plt.ylabel(\"Variance\")\n",
    "plt.show()\n",
    "\n",
    "low_variance_features = feature_variances[feature_variances < 0.1]\n",
    "print(\"\\nLow-variance features:\\n\", low_variance_features)\n",
    "'''\n",
    "# PCA and 2D Visualization\n",
    "pca = PCA(n_components=2)\n",
    "digits_pca = pca.fit_transform(digits.data)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=digits_pca[:, 0], y=digits_pca[:, 1], hue=digits.target, palette='tab10', s=60)\n",
    "plt.title(\"2D PCA Projection of Digits Dataset\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.legend(title=\"Digits\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.show()\n",
    "'''\n",
    "# Distribution of Pixel Values for Digit '0'\n",
    "digit_zero = df_digits[df_digits['Target'] == 0]\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(5):\n",
    "    sns.kdeplot(digit_zero[f'Pixel_{i}'], label=f'Pixel_{i}', fill=True)\n",
    "plt.title(\"Distribution of Pixel Values for Digit '0'\")\n",
    "plt.xlabel(\"Pixel Value\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Pairplot of Selected Pixels\n",
    "selected_pixels = ['Pixel_0', 'Pixel_10', 'Pixel_20', 'Pixel_30', 'Target']\n",
    "sns.pairplot(df_digits[selected_pixels], hue='Target', palette='tab10')\n",
    "plt.suptitle(\"Pairplot of Selected Pixels\", y=1.02, fontsize=14)\n",
    "plt.show()\n",
    "'''\n",
    "# Average Pixel Intensity for Each Digit\n",
    "mean_pixel_intensity = df_digits.groupby('Target').mean().iloc[:, :64]\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 6))\n",
    "for i, ax in enumerate(axes.ravel()):\n",
    "    digit_heatmap = mean_pixel_intensity.iloc[i].values.reshape(8, 8)\n",
    "    sns.heatmap(digit_heatmap, ax=ax, cmap='viridis', cbar=False, annot=False)\n",
    "    ax.set_title(f\"Digit: {i}\")\n",
    "    ax.axis('off')\n",
    "plt.suptitle(\"Average Pixel Intensity for Each Digit\", fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "print('--- CALIFORNIA HOUSING EDA ---')\n",
    "\n",
    "# Load California Housing dataset\n",
    "housing = fetch_california_housing(as_frame=True)\n",
    "X_housing = housing.data\n",
    "y_housing = housing.target\n",
    "df_housing = pd.concat([X_housing, y_housing.rename('MedHouseVal')], axis=1)\n",
    "\n",
    "# Display dataset information\n",
    "'''print(\"\\nColumn Descriptions:\\n\", housing.feature_names + [\"MedHouseVal (Target)\"])\n",
    "df_housing.info()\n",
    "if df_housing.isnull().sum().sum() == 0:\n",
    "    print(\"\\nNo missing values in the dataset!\")'''\n",
    "\n",
    "print(f'\\nMissing Values: {df_housing.isna().sum().sum()}'\n",
    "      f'\\nDuplicates: {df_housing.duplicated().sum()}'\n",
    "      f'\\nShape: {df_housing.shape}')\n",
    "\n",
    "# Apply log transformation to median house value\n",
    "df_housing['MedHouseVal_log'] = np.log1p(df_housing['MedHouseVal'])\n",
    "\n",
    "# Create a new quintile column for Median Income\n",
    "df_housing['MedInc_quintile'] = pd.qcut(df_housing['MedInc'], 5)\n",
    "\n",
    "# Set a more academic style for plots\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"muted\", font_scale=1.2)\n",
    "\n",
    "# Create subplots with a single call\n",
    "fig, axes = plt.subplots(3, 2, figsize=(15, 15))\n",
    "plt.subplots_adjust(hspace=0.7, wspace=0.4)\n",
    "\n",
    "# Flatten axes for easy iteration\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Pre-calculate column names and avoid repeated access to df.columns\n",
    "columns = ['MedInc', 'HouseAge', 'AveRooms', 'AveOccup', 'MedHouseVal']\n",
    "\n",
    "# Loop through columns and plot\n",
    "for i, col in enumerate(columns):\n",
    "    try:\n",
    "        # Drop NaN values and plot the histogram with KDE\n",
    "        sns.histplot(df_housing[col].dropna(), kde=True, bins=25, ax=axes[i])\n",
    "        axes[i].set_title(f'Frequency Distribution of {col}')\n",
    "        axes[i].set_xlabel('Values')\n",
    "        axes[i].set_ylabel('Frequency')\n",
    "\n",
    "    except Exception as e:\n",
    "        # Skip problematic columns without excessive printouts\n",
    "        print(f\"Could not plot {col}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Add the Log-Transformed Median House Value by Median Income Quintiles plot\n",
    "try:\n",
    "    sns.boxplot(\n",
    "        x='MedInc_quintile',\n",
    "        y='MedHouseVal_log',\n",
    "        data=df_housing,\n",
    "        palette=\"muted\",\n",
    "        ax=axes[len(columns)]\n",
    "    )\n",
    "    axes[len(columns)].set_title(\"Log-Transformed Median House Value by Median Income Quintiles\")\n",
    "    axes[len(columns)].set_xlabel(\"Median Income (Quintiles)\")\n",
    "    axes[len(columns)].set_ylabel(\"Log Median House Value ($100,000)\")\n",
    "    axes[len(columns)].tick_params(axis='x', rotation=45)\n",
    "except Exception as e:\n",
    "    print(f\"Could not plot Log-Transformed Median House Value: {e}\")\n",
    "\n",
    "# Hide remaining unused axes\n",
    "for j in range(len(columns) + 1, len(axes)):\n",
    "    axes[j].axis('off')  # This method is generally faster than set_visible(False)\n",
    "\n",
    "df_housing = df_housing.drop(['MedHouseVal_log','MedInc_quintile'], axis=1)\n",
    "\n",
    "# Final adjustments and display\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlation Heatmap\n",
    "correlation_matrix = df_housing.corr()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", cbar=True, vmin=-1, vmax=1)\n",
    "plt.title(\"Correlation Heatmap with Adjusted Scale\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Instantiate and Evaluate Default Classifiers'''\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.3, random_state=42)\n",
    "for model in [DecisionTreeClassifier, RandomForestClassifier]:\n",
    "    pipe = Pipeline([\n",
    "        ('scale', StandardScaler()),\n",
    "        # ('pca', PCA()), PCA Unused due to negative impact on model performance\n",
    "        ('classify',model())],\n",
    "    verbose=True)\n",
    "    predicted = pipe.fit(X_train, y_train).predict(X_test)\n",
    "    print(classification_report(y_test, predicted))\n",
    "    sns.heatmap(confusion_matrix(y_test, predicted))\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Instantiate and Tune Classifiers'''\n",
    "\n",
    "classifiers = {\n",
    "    DecisionTreeClassifier.__name__: {\n",
    "        'model': DecisionTreeClassifier,\n",
    "        'paramSpace': {\n",
    "            'classify__criterion': Categorical(['gini', 'entropy', 'log_loss']),\n",
    "            'classify__splitter': Categorical(['best', 'random']),\n",
    "            'classify__max_depth': Integer(1, 1000),\n",
    "            'classify__min_samples_split': Real(0.01, 0.9),\n",
    "            'classify__min_samples_leaf': Real(0.01, 0.9),\n",
    "            'classify__min_weight_fraction_leaf': Real(0.0,0.5),\n",
    "            'classify__max_features': Real(0.01,0.9),\n",
    "            'classify__max_leaf_nodes': Integer(2, 4000), \n",
    "            'classify__min_impurity_decrease': Real(0.0, 1.0),\n",
    "            'classify__ccp_alpha': Real(0.01, 0.9),\n",
    "            'pca__n_components': Integer(1,len(digits.data[0])),\n",
    "            'scaler__with_mean': [True, False],\n",
    "            'scaler__with_std': [True, False],\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    RandomForestClassifier.__name__: {\n",
    "        'model': RandomForestClassifier,\n",
    "        'paramSpace': {\n",
    "            'classify__n_estimators': Integer(10, 2000),\n",
    "            'classify__criterion': Categorical(['gini', 'entropy', 'log_loss']),\n",
    "            'classify__max_depth': Integer(1, 1000), \n",
    "            'classify__min_samples_split': Real(0.01, 0.9), \n",
    "            'classify__min_samples_leaf': Real(0.01, 0.9),\n",
    "            'classify__min_weight_fraction_leaf': Real(0.01,0.5),\n",
    "            'classify__max_features': Real(0.01,0.9),\n",
    "            'classify__max_leaf_nodes': Integer(1,2000),\n",
    "            'classify__min_impurity_decrease': Real(0.01,0.9),\n",
    "            # 'classify__bootstrap': Categorical([True, False]),\n",
    "            'classify__oob_score': Categorical([True, False]),\n",
    "            'classify__warm_start': Categorical([True, False]),\n",
    "            'classify__max_samples':Real(0.01,0.9),\n",
    "            'pca__n_components': Integer(1,len(digits.data[0])),\n",
    "            'scaler__with_mean': [True, False],\n",
    "            'scaler__with_std': [True, False],\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "tunedModels = {}\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.3, random_state=42)\n",
    "for name, classDict in classifiers.items():\n",
    "    \n",
    "    pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('pca', PCA()),\n",
    "        ('classify',classDict['model']())],\n",
    "    verbose=False)\n",
    "\n",
    "    tunedModels[name] = BayesSearchCV(\n",
    "        pipe,\n",
    "        classDict['paramSpace'],\n",
    "        n_iter= 5,#30, # Reduced for performance during development\n",
    "        cv= 4,#20, # Reduced for performance during development\n",
    "        scoring='accuracy',\n",
    "        # TODO: Use GridSearch for scoring criteria\n",
    "        # NOTE: This will take 13 hours. Execute over night.\n",
    "        random_state=42\n",
    "        )\n",
    "    tunedModels[name].fit(X_train, y_train)\n",
    "    \n",
    "    # search = BayesSearchCV(pipe, param_grid, n_jobs=2)\n",
    "    # predicted = search.best_estimator_.predict(X_test)\n",
    "    predicted = tunedModels[name].best_estimator_.predict(X_test)\n",
    "    \n",
    "    print(classification_report(y_test, predicted))\n",
    "    sns.heatmap(confusion_matrix(y_test, predicted))\n",
    "    plt.title(name + ' Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Evaluate tuning process and resultant models'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Import Regression Data'''\n",
    "cal_housing = fetch_california_housing(as_frame=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Initial data EDA'''\n",
    "\n",
    "outlierTable = PrettyTable(['Feature', 'Outlier Count'])\n",
    "\n",
    "for column in cal_housing.data.columns:\n",
    "    Q1 = cal_housing.data[column].quantile(0.25)\n",
    "    Q3 = cal_housing.data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - 1.5*IQR\n",
    "    upper = Q3 + 1.5*IQR\n",
    "    outlierCount = np.array(cal_housing.data[column] >= upper).sum() + np.array(cal_housing.data[column] <= lower).sum()\n",
    "    outlierTable.add_row([column, outlierCount])\n",
    "    # print(f\"{column}: {outlierCount}\")\n",
    "\n",
    "print(outlierTable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Data Cleaning and Pre-Processing'''\n",
    "cleanData = cal_housing.data\n",
    "cleanData['y'] = cal_housing.target\n",
    "\n",
    "cleanData = cleanData.drop(columns=['Longitude', 'Latitude'])\n",
    "for feature in ['AveBedrms', 'AveRooms', 'AveOccup', 'Population']:\n",
    "    cleanData = cleanData[(np.abs(zscore(cleanData[feature])) < 2.5)]\n",
    "cleanTarget = cleanData['y'].to_list()\n",
    "\n",
    "# DATA CLEANING TODOs\n",
    "# TODO: Bin Lat/Long groupings into city/town clusters. look for available geo-fencing data for cluster labeling - can we do a graph of centroids on top of map?\n",
    "# TODO: Fix Skew for Population, MedIncome, AvgOccup, AvgBedroom, Target\n",
    "# TODO: Feature Engineering / Reduction\n",
    "cleanData.drop(columns=['y'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Instantiate and Evaluate Default Regressors'''\n",
    "modelData = cleanData.copy()\n",
    "\n",
    "# TODO: How does normalization vs standardization impact model performance\n",
    "transformPipeline = [\n",
    "    ('scaler', StandardScaler()),\n",
    "    # ('feature_reduction', PCA(n_components=5,iterated_power=7))\n",
    "    ]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(modelData, cleanTarget, test_size=0.3, random_state=42)\n",
    "for regressor in [DecisionTreeRegressor, RandomForestRegressor]:\n",
    "    pipe = Pipeline(transformPipeline + [('regress',regressor())], verbose=True)\n",
    "    predicted = pipe.fit(X_train, y_train).predict(X_test)\n",
    "    mse = mean_squared_error(y_test, predicted)\n",
    "    print(regressor.__name__)\n",
    "    print(mse)\n",
    "    \n",
    "    \n",
    "    # TODO: Graph Regression Plane using skopt.plots\n",
    "    # NOTE: try using PCA to force data into 3d space\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Instantiate and Evaluate Default Regressors'''\n",
    "modelData = cleanData.copy()\n",
    "\n",
    "# TODO: How does normalization vs standardization impact model performance\n",
    "transformPipeline = [\n",
    "    ('scaler', MinMaxScaler()),\n",
    "    # ('feature_reduction', PCA(n_components=5,iterated_power=7))\n",
    "    ]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(modelData, cleanTarget, test_size=0.3, random_state=42)\n",
    "for regressor in [DecisionTreeRegressor, RandomForestRegressor]:\n",
    "    pipe = Pipeline(transformPipeline + [('regress',regressor())], verbose=True)\n",
    "    predicted = pipe.fit(X_train, y_train).predict(X_test)\n",
    "    mse = mean_squared_error(y_test, predicted)\n",
    "    print(regressor.__name__)\n",
    "    print(mse)\n",
    "    \n",
    "    \n",
    "    # TODO: Graph Regression Plane using skopt.plots\n",
    "    # NOTE: try using PCA to force data into 3d space\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Instantiate and Tune Regressors'''\n",
    "# TODO: Explore how increased demetionality in the parameter space impacts optimization performance\n",
    "regressors = {\n",
    "    DecisionTreeRegressor.__name__: {\n",
    "        'model': DecisionTreeRegressor,\n",
    "        'paramSpace': {\n",
    "            'regress__criterion': ['squared_error', 'friedman_mse', 'absolute_error', 'poisson'],\n",
    "            'regress__splitter': ['best', 'random'],\n",
    "            'regress__max_depth': Integer(2, 1000),\n",
    "            'regress__min_samples_split': Real(0.01, 0.9),\n",
    "            'regress__min_samples_leaf': Real(0.01, 0.9),\n",
    "            'regress__min_weight_fraction_leaf': Real(0.0, 0.5),\n",
    "            'regress__max_features': Real(0.01, 0.5),\n",
    "            'regress__max_leaf_nodes': Integer(2, 1000),\n",
    "            'regress__min_impurity_decrease': Real(0.0, 0.9),\n",
    "            'regress__ccp_alpha': Real(0.01, 0.9),\n",
    "            'pca__n_components': Integer(1,len(modelData.columns)),\n",
    "            'scaler__with_mean': [True, False],\n",
    "            'scaler__with_std': [True, False],\n",
    "        }\n",
    "    },\n",
    "    RandomForestRegressor.__name__: {\n",
    "        'model': RandomForestRegressor,\n",
    "        'paramSpace': {\n",
    "            'regress__n_estimators': Integer(50, 500),\n",
    "            'regress__criterion': Categorical(['squared_error', 'friedman_mse', 'absolute_error', 'poisson']),\n",
    "            'regress__max_depth': Integer(2, 1000), \n",
    "            'regress__min_samples_split': Real(0.01, 0.9),\n",
    "            'regress__min_samples_leaf': Real(0.01, 0.9),\n",
    "            'regress__min_weight_fraction_leaf': Real(0.01, 0.5),\n",
    "            'regress__max_features': Real(0.01,0.9),\n",
    "            # 'regress__max_features': Categorical(['sqrt', 'log2']), \n",
    "            'regress__max_leaf_nodes': Integer(2,1000),\n",
    "            'regress__min_impurity_decrease': Real(0.01, 0.9),\n",
    "            # 'regress__bootstrap': [True, False],\n",
    "            'regress__oob_score': [True, False],\n",
    "            'regress__warm_start': [True, False],\n",
    "            'regress__ccp_alpha': Real(0.01, 0.9),\n",
    "            'regress__max_samples': Real(0.01, 0.9),\n",
    "            'pca__n_components': Integer(1,len(modelData.columns)),\n",
    "            'scaler__with_mean': [True, False],\n",
    "            'scaler__with_std': [True, False],\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "tunedModel = {}\n",
    "X_train, X_test, y_train, y_test = train_test_split(modelData, cleanTarget, test_size=0.3, random_state=42)\n",
    "\n",
    "for name, regDict in regressors.items():\n",
    "    pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('pca', PCA()),\n",
    "        ('regress',regDict['model']())])\n",
    "    tunedModel[name] = BayesSearchCV(\n",
    "        pipe,\n",
    "        regDict['paramSpace'],\n",
    "        n_iter=5,#30,\n",
    "        cv=5,#20,\n",
    "        # scoring=scoreModel\n",
    "        # scoring = scoringCriteria[i]\n",
    "        # TODO: Use GridSearchCV for scoringCriteria param space\n",
    "        # NOTE: This will take 13 hours. Execute over night.\n",
    "        )\n",
    "    tunedModel[name].fit(X_train, y_train)\n",
    "    predicted = tunedModel[name].best_estimator_.predict(X_test)\n",
    "    print('Evaluation Metric:', tunedModel[name].get_params()['scoring'])\n",
    "    print(\"val. score: %s\" % tunedModel[name].best_score_)\n",
    "    print(\"test score: %s\" % tunedModel[name].score(X_test, y_test))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEW CODE BELOW!! - Will"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalers = {'Standardization': StandardScaler(), 'Normalization': MinMaxScaler()}\n",
    "X_train, X_test, y_train, y_test = train_test_split(modelData, cleanTarget, test_size=0.3, random_state=42)\n",
    "\n",
    "for scale_name, scaler in scalers.items():\n",
    "    print(f\"Testing with {scale_name}\")\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    for regressor in [DecisionTreeRegressor, RandomForestRegressor]:\n",
    "        # Define and fit pipeline\n",
    "        pipe = Pipeline([('scaler', scaler), ('regress', regressor())], verbose=True)\n",
    "        predicted = pipe.fit(X_train, y_train).predict(X_test)\n",
    "        y_test_np = np.array(y_test)\n",
    "        \n",
    "        # Plot Actual vs Predicted\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.scatter(y_test, predicted, alpha=0.7, color=\"blue\", label=\"Predicted vs Actual\")\n",
    "        plt.plot([y_test_np.min(), y_test_np.max()], [y_test_np.min(), y_test_np.max()], color=\"red\", lw=2, label=\"Ideal\")\n",
    "        plt.title(f\"Actual vs Predicted - {regressor.__name__}\")\n",
    "        plt.xlabel(\"Actual\")\n",
    "        plt.ylabel(\"Predicted\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "        # Plot Residuals\n",
    "        residuals = y_test - predicted\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.scatter(predicted, residuals, alpha=0.7, color=\"purple\")\n",
    "        plt.axhline(0, color=\"red\", linestyle=\"--\")\n",
    "        plt.title(f\"Residuals - {regressor.__name__}\")\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"Residuals\")\n",
    "        plt.show()\n",
    "\n",
    "        # Apply PCA for 3D Plot\n",
    "        pca = PCA(n_components=2)\n",
    "        X_train_pca = pca.fit_transform(X_train)\n",
    "        X_test_pca = pca.transform(X_test)\n",
    "        \n",
    "        fig = plt.figure(figsize=(10, 7))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        ax.scatter(X_test_pca[:, 0], X_test_pca[:, 1], y_test_np, color='blue', label='Actual')\n",
    "        ax.scatter(X_test_pca[:, 0], X_test_pca[:, 1], predicted, color='green', alpha=0.7, label='Predicted')\n",
    "        ax.set_title(\"3D PCA Regression Plot\")\n",
    "        ax.set_xlabel(\"Principal Component 1\")\n",
    "        ax.set_ylabel(\"Principal Component 2\")\n",
    "        ax.set_zlabel(\"Output\")\n",
    "        ax.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # Calculate and print metrics\n",
    "        print('Mean Absolute Error (MAE):', mean_absolute_error(y_test, predicted))\n",
    "        print('Mean Squared Error (MSE):', mean_squared_error(y_test, predicted))                           # preferred metric\n",
    "        print('Root Mean Squared Error (RMSE):', np.sqrt(mean_squared_error(y_test, predicted)))\n",
    "        print('Mean Absolute Percentage Error (MAPE):', mean_absolute_percentage_error(y_test, predicted))\n",
    "        print('Explained Variance Score:', explained_variance_score(y_test, predicted))\n",
    "        print('Max Error:', max_error(y_test, predicted))\n",
    "        print('Mean Squared Log Error:', mean_squared_log_error(y_test, predicted))\n",
    "        print('Median Absolute Error:', median_absolute_error(y_test, predicted))\n",
    "        print('R^2:', r2_score(y_test, predicted))                                                          # preferred metric\n",
    "        print('Mean Poisson Deviance:', mean_poisson_deviance(y_test, predicted))\n",
    "        print('Mean Gamma Deviance:', mean_gamma_deviance(y_test, predicted))\n",
    "        print('-' * 50)  # Separator between different model results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Metrics\n",
    "classification_metrics = [\n",
    "    'accuracy', 'f1_macro', 'roc_auc', 'balanced_accuracy',\n",
    "    'precision_macro', 'recall_macro', 'average_precision'\n",
    "]\n",
    "\n",
    "# Filter only regression-compatible scoring metrics\n",
    "regression_scoring_criteria = [\n",
    "    'neg_mean_squared_error', 'r2', 'neg_mean_absolute_error',\n",
    "    'neg_mean_squared_log_error', 'neg_median_absolute_error',\n",
    "    'max_error', 'explained_variance', 'neg_root_mean_squared_error'\n",
    "]\n",
    "\n",
    "\n",
    "# Define scoring criteria\n",
    "scoring_criteria = {\n",
    "    'classification': ['accuracy', 'f1_macro', 'roc_auc', 'balanced_accuracy', 'precision_macro', 'recall_macro', 'average_precision'],\n",
    "    'regression': [\n",
    "        'neg_mean_squared_error', 'r2', 'neg_mean_absolute_error', 'neg_mean_squared_log_error', \n",
    "        'neg_median_absolute_error', 'max_error', 'explained_variance', 'neg_root_mean_squared_error'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Regressors parameter spaces for GridSearchCV and RandomizedSearchCV\n",
    "param_spaces = {\n",
    "    'DecisionTreeRegressor': {\n",
    "        'randomized': {\n",
    "            'regress__criterion': ['squared_error', 'friedman_mse', 'absolute_error', 'poisson'],\n",
    "            'regress__splitter': ['best', 'random'],\n",
    "            'regress__max_depth': randint(2, 101),\n",
    "            'regress__min_samples_split': uniform(0.01, 0.89),\n",
    "            'regress__min_samples_leaf': uniform(0.01, 0.89),\n",
    "            'regress__max_features': uniform(0.01, 0.49),\n",
    "            'regress__ccp_alpha': uniform(0.01, 0.89),\n",
    "            'pca__n_components': randint(1, 100),\n",
    "            'scaler__with_mean': [True, False],\n",
    "            'scaler__with_std': [True, False]\n",
    "        },\n",
    "        'grid': {\n",
    "            'regress__splitter': ['best', 'random'],\n",
    "            'regress__max_depth': list(range(2, 1001)),\n",
    "            'regress__min_samples_split': [i / 100 for i in range(1, 91)],\n",
    "            'regress__min_samples_leaf': [i / 100 for i in range(1, 91)],\n",
    "            'regress__max_features': [i / 100 for i in range(1, 51)],\n",
    "            'regress__ccp_alpha': [i / 100 for i in range(1, 91)],\n",
    "            'pca__n_components': list(range(1, 101)),\n",
    "            'scaler__with_mean': [True, False],\n",
    "            'scaler__with_std': [True, False]\n",
    "        }\n",
    "    },\n",
    "    'RandomForestRegressor': {\n",
    "        'randomized': {\n",
    "            'regress__n_estimators': randint(50, 501),\n",
    "            'regress__max_depth': randint(2, 101),\n",
    "            'regress__min_samples_split': uniform(0.01, 0.89),\n",
    "            'regress__min_samples_leaf': uniform(0.01, 0.89),\n",
    "            'regress__max_features': uniform(0.01, 0.89),\n",
    "            'regress__ccp_alpha': uniform(0.01, 0.89),\n",
    "            'regress__max_samples': uniform(0.01, 0.89),\n",
    "            'pca__n_components': randint(1, 100),\n",
    "            'scaler__with_mean': [True, False],\n",
    "            'scaler__with_std': [True, False]\n",
    "        },\n",
    "        'grid': {\n",
    "            'regress__n_estimators': list(range(50, 501)),\n",
    "            'regress__max_depth': list(range(2, 1001)),\n",
    "            'regress__min_samples_split': [i / 100 for i in range(1, 91)],\n",
    "            'regress__min_samples_leaf': [i / 100 for i in range(1, 91)],\n",
    "            'regress__max_features': [i / 100 for i in range(1, 91)],\n",
    "            'regress__ccp_alpha': [i / 100 for i in range(1, 91)],\n",
    "            'regress__max_samples': [i / 100 for i in range(1, 91)],\n",
    "            'pca__n_components': list(range(1, 101)),\n",
    "            'scaler__with_mean': [True, False],\n",
    "            'scaler__with_std': [True, False]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Function to perform GridSearchCV or RandomizedSearchCV\n",
    "def tune_model(search_type, model_name, model, param_space, X_train, y_train, X_test, y_test):\n",
    "    pipe = Pipeline([ \n",
    "        ('scaler', StandardScaler()), \n",
    "        ('pca', PCA()), \n",
    "        ('regress', model()) \n",
    "    ])\n",
    "\n",
    "    # Set search class based on the search_type\n",
    "    search_class = RandomizedSearchCV if search_type == 'randomized' else GridSearchCV\n",
    "\n",
    "    # Set parameters based on search type\n",
    "    search_params = {\n",
    "        'estimator': pipe,\n",
    "        'n_jobs': -1,\n",
    "        'cv': 5,\n",
    "        'scoring': None,\n",
    "        'return_train_score': True\n",
    "    }\n",
    "\n",
    "    # For RandomizedSearchCV, use 'param_distributions' and set 'n_iter'\n",
    "    if search_type == 'randomized':\n",
    "        search_params['n_iter'] = 50  # Only for RandomizedSearchCV\n",
    "        search_params['param_distributions'] = param_space  # Use for RandomizedSearchCV\n",
    "    else:\n",
    "        search_params['param_grid'] = param_space  # Use param_grid for GridSearchCV\n",
    "\n",
    "    search = search_class(**search_params)\n",
    "\n",
    "    # Loop over scoring metrics for evaluation\n",
    "    for scoring_metric in scoring_criteria['regression']:\n",
    "        print(f'Tuning {model_name} with {search_type} search and scoring metric: {scoring_metric}')\n",
    "        search.set_params(scoring=scoring_metric)\n",
    "        search.fit(X_train, y_train)\n",
    "        \n",
    "        print(f'Evaluation Metric: {scoring_metric}')\n",
    "        print(f\"Best Score (Train): {search.best_score_}\")\n",
    "        print(f\"Test Score: {search.score(X_test, y_test)}\")\n",
    "        print('-' * 80)\n",
    "\n",
    "    return search\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(modelData, cleanTarget, test_size=0.3, random_state=42)\n",
    "\n",
    "# Instantiate tuned models dictionary\n",
    "tuned_models = {}\n",
    "\n",
    "# Iterate through each regressor and perform tuning\n",
    "for model_name, model_dict in param_spaces.items():\n",
    "    # Run RandomizedSearchCV first\n",
    "    if 'randomized' in model_dict:\n",
    "        param_space = model_dict['randomized']\n",
    "        search = tune_model('randomized', model_name, globals()[model_name], param_space, X_train, y_train, X_test, y_test)\n",
    "        tuned_models[model_name] = search\n",
    "\n",
    "    # Then run GridSearchCV\n",
    "    if 'grid' in model_dict:\n",
    "        param_space = model_dict['grid']\n",
    "        search = tune_model('grid', model_name, globals()[model_name], param_space, X_train, y_train, X_test, y_test)\n",
    "        tuned_models[model_name] = search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = opt.best_estimator_.steps[-1][1]\n",
    "features = modelData.columns\n",
    "importance = regressor.feature_importances_\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(list(modelData.columns), importance)\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Evaluate tuning process and resultant models'''\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
