{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Imports'''\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "'''Import Utils'''\n",
    "\n",
    "\n",
    "'''Import Data'''\n",
    "from sklearn.datasets import load_digits, fetch_california_housing\n",
    "\n",
    "'''Import Data Processing Utilities'''\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, mean_squared_error, f1_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "'''Import Predictors'''\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "'''Import Tuning Utilities'''\n",
    "from skopt.space import Integer\n",
    "from skopt.space import Real\n",
    "from skopt.space import Categorical\n",
    "from skopt.utils import use_named_args\n",
    "from skopt import gp_minimize\n",
    "from skopt.plots import plot_objective, plot_histogram, plot_convergence, plot_gaussian_process\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Global Variables'''\n",
    "\n",
    "scoringCriteria = [\n",
    "    'positive_likelihood_ratio', 'f1_micro', 'v_measure_score', 'roc_auc_ovr_weighted',\n",
    "    'adjusted_rand_score', 'neg_mean_squared_log_error', 'r2', 'precision_macro',\n",
    "    'roc_auc_ovo_weighted', 'neg_median_absolute_error', 'matthews_corrcoef', 'precision_samples',\n",
    "    'jaccard_micro', 'balanced_accuracy', 'precision_micro', 'recall_micro', 'f1_samples',\n",
    "    'recall', 'jaccard', 'neg_mean_poisson_deviance', 'rand_score', 'jaccard_weighted',\n",
    "    'neg_mean_absolute_percentage_error', 'precision_weighted', 'average_precision',\n",
    "    'neg_log_loss', 'recall_samples', 'recall_weighted', 'accuracy', 'homogeneity_score',\n",
    "    'neg_mean_absolute_error', 'adjusted_mutual_info_score', 'roc_auc', 'completeness_score',\n",
    "    'f1_macro', 'roc_auc_ovo', 'recall_macro', 'top_k_accuracy', 'f1', 'roc_auc_ovr',\n",
    "    'normalized_mutual_info_score', 'fowlkes_mallows_score', 'mutual_info_score', 'explained_variance',\n",
    "    'max_error', 'neg_negative_likelihood_ratio', 'neg_brier_score', 'neg_mean_squared_error',\n",
    "    'f1_weighted', 'neg_root_mean_squared_error', 'jaccard_samples', 'neg_mean_gamma_deviance',\n",
    "    'jaccard_macro', 'precision'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Classifier data EDA'''\n",
    "digits = load_digits()\n",
    "# print(digits.keys())\n",
    "# print(digits['data'][0])\n",
    "# print(digits.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Default Classifier Performance'''\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.3, random_state=42)\n",
    "for model in [DecisionTreeClassifier, RandomForestClassifier]:\n",
    "    pipe = Pipeline([\n",
    "        ('scale', StandardScaler()),\n",
    "        # ('pca', PCA()),\n",
    "        ('classify',model())],\n",
    "    verbose=True)\n",
    "    predicted = pipe.fit(X_train, y_train).predict(X_test)\n",
    "    print(classification_report(y_test, predicted))\n",
    "    sns.heatmap(confusion_matrix(y_test, predicted))\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Tuned Classifier Performance'''\n",
    "\n",
    "# TODO: Model Tuning \n",
    "# Update the below to implement Bayesian Optimization model tuning.\n",
    "# Use skopt.gp_minimize() vis a vi: https://machinelearningmastery.com/scikit-optimize-for-hyperparameter-tuning-in-machine-learning/\n",
    "def f1(estimator, X_test, y_test):\n",
    "    return f1_score(y_test, estimator.predict(X_test))\n",
    "\n",
    "classifiers = {\n",
    "    DecisionTreeClassifier.__name__: {\n",
    "        'model': DecisionTreeClassifier,\n",
    "        'paramSpace': {\n",
    "            'classify__criterion': Categorical(['gini', 'entropy', 'log_loss']),\n",
    "            'classify__splitter': Categorical(['best', 'random']),\n",
    "            'classify__max_depth': Integer(1, 1000),\n",
    "            'classify__min_samples_split': Real(0.01, 0.9),\n",
    "            'classify__min_samples_leaf': Real(0.01, 0.9),\n",
    "            'classify__min_weight_fraction_leaf': Real(0.0,0.5),\n",
    "            'classify__max_features': Real(0.01,0.9),\n",
    "            'classify__max_leaf_nodes': Integer(2, 4000), \n",
    "            'classify__min_impurity_decrease': Real(0.0, 1.0),\n",
    "            'classify__ccp_alpha': Real(0.01, 0.9),\n",
    "            'pca__n_components': Integer(1,len(digits.data[0])),\n",
    "            'scaler__with_mean': [True, False],\n",
    "            'scaler__with_std': [True, False],\n",
    "        }\n",
    "    },\n",
    "    RandomForestClassifier.__name__: {\n",
    "        'model': RandomForestClassifier,\n",
    "        'paramSpace': {\n",
    "            'classify__n_estimators': Integer(10, 2000),\n",
    "            'classify__criterion': Categorical(['gini', 'entropy', 'log_loss']),\n",
    "            'classify__max_depth': Integer(1, 1000), \n",
    "            'classify__min_samples_split': Real(0.01, 0.9), \n",
    "            'classify__min_samples_leaf': Real(0.01, 0.9),\n",
    "            'classify__min_weight_fraction_leaf': Real(0.01,0.5),\n",
    "            'classify__max_features': Real(0.01,0.9),\n",
    "            'classify__max_leaf_nodes': Integer(1,2000),\n",
    "            'classify__min_impurity_decrease': Real(0.01,0.9),\n",
    "            # 'classify__bootstrap': Categorical([True, False]),\n",
    "            'classify__oob_score': Categorical([True, False]),\n",
    "            'classify__warm_start': Categorical([True, False]),\n",
    "            'classify__max_samples':Real(0.01,0.9),\n",
    "            'pca__n_components': Integer(1,len(digits.data[0])),\n",
    "            'scaler__with_mean': [True, False],\n",
    "            'scaler__with_std': [True, False],\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "tunedModel = {}\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.3, random_state=42)\n",
    "for name, classDict in classifiers.items():\n",
    "    \n",
    "    pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('pca', PCA()),\n",
    "        ('classify',classDict['model']())],\n",
    "    verbose=False)\n",
    "\n",
    "    tunedModel[name] = BayesSearchCV(\n",
    "        pipe,\n",
    "        classDict['paramSpace'],\n",
    "        n_iter=30,\n",
    "        cv=20,\n",
    "        scoring='accuracy'\n",
    "        # TODO: Use GridSearch for scoring criteria\n",
    "        # NOTE: This will take 13 hours. Execute over night.\n",
    "        # random_state=42 \n",
    "        )\n",
    "    tunedModel[name].fit(X_train, y_train)\n",
    "    \n",
    "    # search = BayesSearchCV(pipe, param_grid, n_jobs=2)\n",
    "    # predicted = search.best_estimator_.predict(X_test)\n",
    "    predicted = tunedModel[name].best_estimator_.predict(X_test)\n",
    "    \n",
    "    print(classification_report(y_test, predicted))\n",
    "    sns.heatmap(confusion_matrix(y_test, predicted))\n",
    "    plt.title(name + ' Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model in tunedModel.items():\n",
    "    _ = plot_objective(model.optimizer_results_[0])\n",
    "    plt.subplots_adjust(wspace=0.5, hspace=0.5)\n",
    "    plt.show()\n",
    "    _ = plot_convergence(model.optimizer_results_[0])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Regressor data EDA'''\n",
    "cal_housing = fetch_california_housing(as_frame=True)\n",
    "# print(cal_housing.data)\n",
    "# print(cal_housing.target)\n",
    "\n",
    "outlierTable = PrettyTable(['Feature', 'Outlier Count'])\n",
    "\n",
    "for column in cal_housing.data.columns:\n",
    "    Q1 = cal_housing.data[column].quantile(0.25)\n",
    "    Q3 = cal_housing.data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - 1.5*IQR\n",
    "    upper = Q3 + 1.5*IQR\n",
    "    outlierCount = np.array(cal_housing.data[column] >= upper).sum() + np.array(cal_housing.data[column] <= lower).sum()\n",
    "    outlierTable.add_row([column, outlierCount])\n",
    "    # print(f\"{column}: {outlierCount}\")\n",
    "\n",
    "print(outlierTable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Data Cleaning and Pre-Processing'''\n",
    "cleanData = cal_housing.data\n",
    "cleanData['y'] = cal_housing.target\n",
    "\n",
    "cleanData = cleanData.drop(columns=['Longitude', 'Latitude'])\n",
    "for feature in ['AveBedrms', 'AveRooms', 'AveOccup', 'Population']:\n",
    "    cleanData = cleanData[(np.abs(stats.zscore(cleanData[feature])) < 2.5)]\n",
    "cleanTarget = cleanData['y'].to_list()\n",
    "\n",
    "# DATA CLEANING TODOs\n",
    "# TODO: Bin Lat/Long groupings into city/town clusters. look for available geo-fencing data for cluster labeling - can we do a graph of centroids on top of map?\n",
    "# TODO: Fix Skew for Population, MedIncome, AvgOccup, AvgBedroom, Target\n",
    "# TODO: Feature Engineering / Reduction\n",
    "cleanData.drop(columns=['y'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Plot clean features to explore relationships'''\n",
    "plotData = cleanData.copy()\n",
    "plotData['y'] = cleanTarget.copy()\n",
    "\n",
    "# sns.pairplot(plotData, hue='y')\n",
    "sns.pairplot(plotData.drop(columns=['y']), kind='reg', plot_kws={'line_kws':{'color':'red'}})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Show distribution of Target Value'''\n",
    "sns.histplot(data=plotData, x='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Plot data by Bin'''\n",
    "bin1 = plotData[plotData['y'] < 1]\n",
    "bin2 = plotData[plotData['y'] > 1][plotData['y'] <= 2]\n",
    "bin3 = plotData[plotData['y'] > 2][plotData['y'] <= 3]\n",
    "bin4 = plotData[plotData['y'] > 3][plotData['y'] <= 4]\n",
    "bin5 = plotData[plotData['y'] > 4]\n",
    "\n",
    "for index, bin in enumerate([bin1, bin2, bin3, bin4, bin5]):\n",
    "    fig, axs = plt.subplots(ncols=2, nrows=2)\n",
    "    plt.subplots_adjust(wspace=0.3, hspace=0.3)\n",
    "    fig.suptitle('Bin'+str(index)+' - Pop '+str(len(bin)))\n",
    "    sns.histplot(x='MedInc', data=bin, ax=axs[0,0])\n",
    "    axs[0,0].set_ylim(bottom=0, top=800)\n",
    "    axs[0,0].set_xlim(left=0, right=16)\n",
    "    sns.regplot(x='MedInc', y='AveRooms', data=bin, ax=axs[0,1], line_kws={'color':'red'})\n",
    "    axs[0,1].set_ylim(bottom=0, top=10)\n",
    "    axs[0,1].set_xlim(left=0, right=16)\n",
    "    sns.regplot(x='MedInc', y='AveBedrms', data=bin, ax=axs[1,0], line_kws={'color':'red'})\n",
    "    axs[1,0].set_ylim(bottom=0, top=3.0)\n",
    "    axs[1,0].set_xlim(left=0, right=16)\n",
    "    # sns.countplot(x='MedInc', data=bin, ax=axs[1,1])\n",
    "    sns.regplot(x='MedInc', y='AveOccup', data=bin, ax=axs[1,1], line_kws={'color':'red'})\n",
    "\n",
    "    axs[1,1].set_ylim(bottom=0, top=25)\n",
    "    axs[1,1].set_xlim(left=0, right=16)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Default Regressor Performance'''\n",
    "modelData = cleanData.copy()\n",
    "\n",
    "# TODO: How does normalization vs standardization impact model performance\n",
    "transformPipeline = [\n",
    "    ('scaler', StandardScaler()),\n",
    "    # ('feature_reduction', PCA(n_components=5,iterated_power=7))\n",
    "    ]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(modelData, cleanTarget, test_size=0.3, random_state=42)\n",
    "for regressor in [DecisionTreeRegressor, RandomForestRegressor]:\n",
    "    pipe = Pipeline(transformPipeline + [('regress',regressor())], verbose=True)\n",
    "    predicted = pipe.fit(X_train, y_train).predict(X_test)\n",
    "    mse = mean_squared_error(y_test, predicted)\n",
    "    print(regressor.__name__)\n",
    "    print(mse)\n",
    "    \n",
    "    \n",
    "    # TODO: Graph Regression Plane using skopt.plots\n",
    "    # NOTE: PCA to force data into 3d space\n",
    "    # feat1 = [x for x,_,_ in newX]\n",
    "    # feat2 = [x for _,x,_ in newX]\n",
    "    # feat3 = [x for _,_,x in newX]\n",
    "    \n",
    "    # fig = plt.figure()\n",
    "    # ax = fig.add_subplot(projection='3d')\n",
    "    \n",
    "    # ax.scatter(feat1, feat2, feat3, label='Data')\n",
    "    # ax.plot(y_test, predicted, color='red', label='Regression Line')\n",
    "    # ax.set_xlabel('PCA 1')\n",
    "    # ax.set_ylabel('PCA 2')\n",
    "    # ax.set_zlabel('PCA 3')\n",
    "    # ax.legend()\n",
    "    # plt.title((f'Linear Regression (MSE: {mse:.2f})'))\n",
    "    # plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Tuned Regressor Performance'''\n",
    "# TODO: Explore how increased demetionality in the parameter space impacts optimization performance\n",
    "regressors = {\n",
    "    DecisionTreeRegressor.__name__: {\n",
    "        'model': DecisionTreeRegressor,\n",
    "        'paramSpace': {\n",
    "            'regress__criterion': ['squared_error', 'friedman_mse', 'absolute_error', 'poisson'],\n",
    "            'regress__splitter': ['best', 'random'],\n",
    "            'regress__max_depth': Integer(2, 1000),\n",
    "            'regress__min_samples_split': Real(0.01, 0.9),\n",
    "            'regress__min_samples_leaf': Real(0.01, 0.9),\n",
    "            'regress__min_weight_fraction_leaf': Real(0.0, 0.5),\n",
    "            'regress__max_features': Real(0.01, 0.5),\n",
    "            'regress__max_leaf_nodes': Integer(2, 1000),\n",
    "            'regress__min_impurity_decrease': Real(0.0, 0.9),\n",
    "            'regress__ccp_alpha': Real(0.01, 0.9),\n",
    "            'pca__n_components': Integer(1,len(modelData.columns)),\n",
    "            'scaler__with_mean': [True, False],\n",
    "            'scaler__with_std': [True, False],\n",
    "        }\n",
    "    },\n",
    "    RandomForestRegressor.__name__: {\n",
    "        'model': RandomForestRegressor,\n",
    "        'paramSpace': {\n",
    "            'regress__n_estimators': Integer(50, 500),\n",
    "            'regress__criterion': Categorical(['squared_error', 'friedman_mse', 'absolute_error', 'poisson']),\n",
    "            'regress__max_depth': Integer(2, 1000), \n",
    "            'regress__min_samples_split': Real(0.01, 0.9),\n",
    "            'regress__min_samples_leaf': Real(0.01, 0.9),\n",
    "            'regress__min_weight_fraction_leaf': Real(0.01, 0.5),\n",
    "            'regress__max_features': Real(0.01,0.9),\n",
    "            # 'regress__max_features': Categorical(['sqrt', 'log2']), \n",
    "            'regress__max_leaf_nodes': Integer(2,1000),\n",
    "            'regress__min_impurity_decrease': Real(0.01, 0.9),\n",
    "            # 'regress__bootstrap': [True, False],\n",
    "            'regress__oob_score': [True, False],\n",
    "            'regress__warm_start': [True, False],\n",
    "            'regress__ccp_alpha': Real(0.01, 0.9),\n",
    "            'regress__max_samples': Real(0.01, 0.9),\n",
    "            'pca__n_components': Integer(1,len(modelData.columns)),\n",
    "            'scaler__with_mean': [True, False],\n",
    "            'scaler__with_std': [True, False],\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "tunedModel = {}\n",
    "X_train, X_test, y_train, y_test = train_test_split(modelData, cleanTarget, test_size=0.3, random_state=42)\n",
    "\n",
    "for name, regDict in regressors.items():\n",
    "    pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('pca', PCA()),\n",
    "        ('regress',regDict['model']())])\n",
    "    tunedModel[name] = BayesSearchCV(\n",
    "        pipe,\n",
    "        regDict['paramSpace'],\n",
    "        n_iter=30,\n",
    "        cv=20,\n",
    "        # scoring=scoreModel\n",
    "        # scoring = scoringCriteria[i]\n",
    "        # TODO: Use GridSearchCV for scoringCriteria param space\n",
    "        # NOTE: This will take 13 hours. Execute over night.\n",
    "        )\n",
    "    tunedModel[name].fit(X_train, y_train)\n",
    "    predicted = tunedModel[name].best_estimator_.predict(X_test)\n",
    "    print('Evaluation Metric:', tunedModel[name].get_params()['scoring'])\n",
    "    print(\"val. score: %s\" % tunedModel[name].best_score_)\n",
    "    print(\"test score: %s\" % tunedModel[name].score(X_test, y_test))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Explore Optimizer'''\n",
    "# tunedModel[DecisionTreeRegressor.__name__].optimizer_results_\n",
    "# .models[-1]\n",
    "# print([x for x in regressors[DecisionTreeRegressor.__name__]['paramSpace'].keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Plot Optimization'''\n",
    "for name, model in tunedModel.items():\n",
    "    _ = plot_objective(result=model.optimizer_results_[0])\n",
    "    plt.subplots_adjust(wspace=0.5, hspace=0.5)\n",
    "    plt.show()\n",
    "    _ = plot_convergence(model.optimizer_results_[0])\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
